{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Document QA - Code Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the workspace used to create the functions used in this code base, which can help a user understand certain components of the program and verify their functionality. I've gone through and added markdown and comments to explain the development though process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object to handle our communications with the LLM. We'll use GPT here - you'll need to have an access token saved to a txt file in order to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ACCESS_TOKEN_PATH = os.path.pardir + \"/api_keys/openai.key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import logging\n",
    "\n",
    "# Create a parent class as an abstract \n",
    "\n",
    "\n",
    "class GPTCommunicator():\n",
    "\n",
    "    def __init__(\n",
    "            self, api_key_path: str, model_name: str = \"gpt-3.5-turbo\",\n",
    "        ):\n",
    "\n",
    "        # init client with api key file\n",
    "        with open(api_key_path) as f:\n",
    "            self.client = OpenAI(api_key=f.readline().strip())\n",
    "        \n",
    "        # context window limits; found at https://platform.openai.com/docs/models\n",
    "        model_max_tokens = { \n",
    "            #\"gpt-3.5-turbo-instruct\": 4096,\n",
    "            \"gpt-3.5-turbo\": 16385,\n",
    "            \"gpt-4\": 8192,\n",
    "            \"gpt-4-32k\": 32768,\n",
    "        }\n",
    "\n",
    "        # check for valid model name input\n",
    "        if model_name not in model_max_tokens.keys():\n",
    "            raise ValueError(f\"Invalid model name; valid args include: {model_max_tokens.keys()}\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # set model attributes\n",
    "        self.max_prompt_tokens = model_max_tokens[model_name] -  250 # buffer for response tokens\n",
    "        self.system_role = \"You are a helpful AI assistant.\"\n",
    "        self.total_tokens_used = 0\n",
    "        \n",
    "        \n",
    "    def post_prompt(self, text: str):\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model = self.model_name,\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": str(self.system_role)},\n",
    "                    {\"role\": \"user\", \"content\": str(text)}\n",
    "                ]\n",
    "            )\n",
    "            self.last_response = response\n",
    "            self.total_tokens_used += int(response.usage.total_tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to post prompt: {e}\")\n",
    "            return None\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "\n",
    "        encoding = tiktoken.encoding_for_model(self.model_name)\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "\n",
    "        return num_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTCommunicator(ACCESS_TOKEN_PATH)\n",
    "\n",
    "# test communication\n",
    "response = gpt.post_prompt(\"Hello\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.total_tokens_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=9, prompt_tokens=19, total_tokens=28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.last_response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.count_tokens(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Europe is a continent located in the Northern Hemisphere and is bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, and the Mediterranean Sea to the south. It is known for its rich history, diverse cultures, and stunning landscapes. Europe is home to many iconic landmarks, such as the Eiffel Tower, Colosseum, and Big Ben.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(\"Describe Europe in 3 sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.total_tokens_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=75, prompt_tokens=25, total_tokens=100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.last_response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/anaconda3/envs/LLM_WORKSPACE/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
       " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       " '',\n",
       " ' = = Gameplay = = \\n',\n",
       " '',\n",
       " \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext[\"train\"][\"text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is split into a list of strings. We want to combine the strings to form full articles as individual documents. Based on visual inspection, it seems a delimiter \" = \" on both sides of a text is used to mark titles, \" = = \" for headers and \" = = = \" for sub-headers. We can group the text based on thr start of a new title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# confirm \" = = = \" is the lowest category\n",
    "\n",
    "np.max([int(t.count(\" = \") / 2) for t in wikitext[\"train\"][\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_string_type(text):\n",
    "    if text == '':\n",
    "        return \"empty\"\n",
    "    \n",
    "    title_delimiter = \" = \"\n",
    "    header_delimiter = \" = = \"\n",
    "    subheader_delimiter = \" = = = \"\n",
    "\n",
    "    def check_by_delimiter(t, delimiter):\n",
    "        # when split by the right delimiter, text will be in the form: ['', text, '\\n']\n",
    "        t_split = t.split(delimiter)\n",
    "\n",
    "        # for titles and headers, we can expect split == 3 and split[-1] == \\n\n",
    "        if len(t_split) == 3 and t_split[-1] == '\\n':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    if check_by_delimiter(text, subheader_delimiter):\n",
    "        return \"subheader\"\n",
    "    \n",
    "    elif check_by_delimiter(text, header_delimiter):\n",
    "        return \"header\"\n",
    "    \n",
    "    elif check_by_delimiter(text, title_delimiter):\n",
    "        return \"title\"\n",
    "    \n",
    "    else:\n",
    "        return \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = wikitext[\"train\"][\"text\"][1]\n",
    "print(text)\n",
    "classify_string_type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type\n",
       "0                                                        empty\n",
       "1                     = Valkyria Chronicles III = \\n     title\n",
       "2                                                        empty\n",
       "3   Senjō no Valkyria 3 : Unrecorded Chronicles (...   content\n",
       "4   The game began development in 2010 , carrying...   content"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_list = wikitext[\"train\"][\"text\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = text_list\n",
    "df[\"text_type\"] = list(map(lambda t: classify_string_type(t), text_list))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_type\n",
       "content      17870\n",
       "empty        12951\n",
       "header        2922\n",
       "subheader     2346\n",
       "title          629\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It met with positive sales in Japan , and was...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>= = Gameplay = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As with previous Valkyira Chronicles games , ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The game 's battle system , the BliTZ system ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Troops are divided into five classes : Scouts...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>= = Plot = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The game takes place during the Second Europa...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>As the Nameless officially do not exist , the...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Partly due to these events , and partly due t...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>= = Development = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Concept work for Valkyria Chronicles III bega...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The majority of material created for previous...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>= = = Music = = = \\n</td>\n",
       "      <td>subheader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The music was composed by Hitoshi Sakimoto , ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>= = = Release = = = \\n</td>\n",
       "      <td>subheader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>In September 2010 , a teaser website was reve...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Unlike its two predecessors , Valkyria Chroni...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>= = Reception = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>On its day of release in Japan , Valkyria Chr...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Famitsu enjoyed the story , and were particul...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>PlayStation Official Magazine - UK praised th...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>In a preview of the TGS demo , Ryan Geddes of...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>= = Legacy = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Kurt and Riela were featured in the Nintendo ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>= = = Adaptations = = = \\n</td>\n",
       "      <td>subheader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Valkyria Chronicles 3 was adapted into a two ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>The anime 's title was inspired by the princi...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Two manga adaptations were produced , followi...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal = \\n</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  text_type\n",
       "1                      = Valkyria Chronicles III = \\n      title\n",
       "2                                                          empty\n",
       "3    Senjō no Valkyria 3 : Unrecorded Chronicles (...    content\n",
       "4    The game began development in 2010 , carrying...    content\n",
       "5    It met with positive sales in Japan , and was...    content\n",
       "6                                                          empty\n",
       "7                                 = = Gameplay = = \\n     header\n",
       "8                                                          empty\n",
       "9    As with previous Valkyira Chronicles games , ...    content\n",
       "10   The game 's battle system , the BliTZ system ...    content\n",
       "11   Troops are divided into five classes : Scouts...    content\n",
       "12                                                         empty\n",
       "13                                    = = Plot = = \\n     header\n",
       "14                                                         empty\n",
       "15   The game takes place during the Second Europa...    content\n",
       "16   As the Nameless officially do not exist , the...    content\n",
       "17   Partly due to these events , and partly due t...    content\n",
       "18                                                         empty\n",
       "19                             = = Development = = \\n     header\n",
       "20                                                         empty\n",
       "21   Concept work for Valkyria Chronicles III bega...    content\n",
       "22   The majority of material created for previous...    content\n",
       "23                                                         empty\n",
       "24                               = = = Music = = = \\n  subheader\n",
       "25                                                         empty\n",
       "26   The music was composed by Hitoshi Sakimoto , ...    content\n",
       "27                                                         empty\n",
       "28                             = = = Release = = = \\n  subheader\n",
       "29                                                         empty\n",
       "30   In September 2010 , a teaser website was reve...    content\n",
       "31   Unlike its two predecessors , Valkyria Chroni...    content\n",
       "32                                                         empty\n",
       "33                               = = Reception = = \\n     header\n",
       "34                                                         empty\n",
       "35   On its day of release in Japan , Valkyria Chr...    content\n",
       "36   Famitsu enjoyed the story , and were particul...    content\n",
       "37   PlayStation Official Magazine - UK praised th...    content\n",
       "38   In a preview of the TGS demo , Ryan Geddes of...    content\n",
       "39                                                         empty\n",
       "40                                  = = Legacy = = \\n     header\n",
       "41                                                         empty\n",
       "42   Kurt and Riela were featured in the Nintendo ...    content\n",
       "43                                                         empty\n",
       "44                         = = = Adaptations = = = \\n  subheader\n",
       "45                                                         empty\n",
       "46   Valkyria Chronicles 3 was adapted into a two ...    content\n",
       "47   The anime 's title was inspired by the princi...    content\n",
       "48   Two manga adaptations were produced , followi...    content\n",
       "49                                                         empty\n",
       "50                                                         empty\n",
       "51   = Tower Building of the Little Rock Arsenal = \\n      title"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_idx = df.index[df['text_type']==\"title\"].tolist()\n",
    "df.iloc[title_idx[0]:title_idx[1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "\n",
      "\n",
      " = = Gameplay = = \n",
      "\n",
      "\n",
      " As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements r\n"
     ]
    }
   ],
   "source": [
    "passage = \"\\n\".join(df.iloc[title_idx[0]:title_idx[1]][\"text\"])\n",
    "print(passage[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ation 4 that forms the beginning of a new series within the Valkyria franchise . \n",
      "\n",
      "\n",
      " = = = Adaptations = = = \n",
      "\n",
      "\n",
      " Valkyria Chronicles 3 was adapted into a two @-@ episode original video animation series in the same year of its release . Titled Senjō no Valkyria 3 : Taga Tame no Jūsō ( 戦場のヴァルキュリア３ 誰がための銃瘡 , lit . Valkyria of the Battlefield 3 : The Wound Taken for Someone 's Sake ) , it was originally released through PlayStation Network and Qriocity between April and May 2011 . The initially @-@ planned release and availability period needed to be extended due to a stoppage to PSN during the early summer of that year . It later released for DVD on June 29 and August 31 , 2011 , with separate \" Black \" and \" Blue \" editions being available for purchase . The anime is set during the latter half of Valkyria Chronicles III , detailing a mission by the Nameless against their Imperial rivals Calamity Raven . The anime was first announced in November 2010 . It was developed by A @-@ 1 Pictures , produced by Shinji Motoyama , directed by Nobuhiro Kondō , and written by Hiroshi Ōnogi . Sakimoto 's music for the game was used in the anime . \n",
      "\n",
      " The anime 's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of others . A subtitle attached to the project during development was \" The Road to Kubinka \" , which referenced the Kubinka Tank Museum in Moscow . The game 's main theme was how the characters regained their sense of self when stripped of their names and identities , along with general themes focused on war and its consequences . While making the anime , the production team were told by Sega to make it as realistic as possible , with the consequence that the team did extensive research into aspects such as what happened when vehicles like tanks were overturned or damaged . Due to it being along the same timeline as the original game and its television anime adaptation , the cast of Valkyria Chronicles could make appearances , which pleased the team . The opening theme , \" Akari ( Light ) -Tomoshibi- \" ( 灯 @-@ TOMOSHIBI- ) , was sung by Japanese singer Faylan . The ending theme , \" Someday the Flowers of Light Will Bloom \" ( いつか咲く光の花 , Itsuka Saku Hikari no Hana ) , was sung by Minami Kuribayashi . Both songs ' lyrics were written by their respective artists . \n",
      "\n",
      " Two manga adaptations were produced , following each of the game 's main female protagonists Imca and Riela . They were Senjō no Valkyria 3 : Namo naki Chikai no Hana ( 戦場のヴァルキュリア3 名もなき誓いの花 , lit . Valkyria of the Battlefield 3 : The Flower of the Nameless Oath ) , illustrated by Naoyuki Fujisawa and eventually released in two volumes after being serialized in Dengeki Maoh between 2011 and 2012 ; and Senjō no Valkyria 3 : -Akaki Unmei no Ikusa Otome- ( 戦場のヴァルキュリア3 -赤き運命の戦乙女- , lit . Valkyria of the Battlefield 3 -The Valkyrie of the Crimson Fate ) , illustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(passage[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform list of single text lines into list of passages. These passages will be saved and used as documents for our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア ... ustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def segment_into_passages(text_list):\n",
    "\n",
    "    text_type = list(map(lambda t: classify_string_type(t), text_list))\n",
    "\n",
    "    type_counts = Counter(text_type) #dict storing counts of titles, headers, etc.\n",
    "\n",
    "    title_idx = np.array([i for i,v in enumerate(text_type) if v == \"title\"])\n",
    "    title_idx = np.append(title_idx, len(text_list)) #append for last passage\n",
    "    title_idx_pairs = np.column_stack((title_idx[:-1], title_idx[1:]))\n",
    "\n",
    "    passages = []\n",
    "\n",
    "    for idx_pair in title_idx_pairs:\n",
    "        start_i, end_i = idx_pair[0], idx_pair[1]\n",
    "        passage = \"\\n\".join(text_list[start_i:end_i])\n",
    "        passages.append(passage)\n",
    "\n",
    "    assert len(passages) == type_counts[\"title\"], \"Passage count should match number of titles\"\n",
    "\n",
    "    return passages\n",
    "\n",
    "passages = segment_into_passages(text_list)\n",
    "print(f\"{passages[0][:100]} ... {passages[0][-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Tower Building of the Little Rock Arsenal = \n",
      "\n",
      "\n",
      " The Tower Building of the Little Rock Arsenal , a ... emen and servicewomen of the United States and commemorate the birthplace of Douglas MacArthur . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{passages[1][:100]} ... {passages[1][-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y , which had not yet been performed in public . He became very attached to the bird and arranged an elaborate funeral for it when it died three years later . It has been suggested that his A Musical Joke ( K. 522 ) might be written in the comical , inconsequential style of a starling 's vocalisation . Other people who have owned common starlings report how adept they are at picking up phrases and expressions . The words have no meaning for the starling , so they often mix them up or use them on what to humans are inappropriate occasions in their songs . Their ability at mimicry is so great that strangers have looked in vain for the human they think they have just heard speak . \n",
      "\n",
      " Common starlings are trapped for food in some Mediterranean countries . The meat is tough and of low quality , so it is casseroled or made into pâté . One recipe said it should be stewed \" until tender , however long that may be \" . Even when correctly prepared , it may still be seen as an acquired taste . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify against text_list\n",
    "\n",
    "print(passages[-1][-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Western Australia banned the import of common starlings in 1895 . New flocks arriving from the east are routinely shot , while the less cautious juveniles are trapped and netted . New methods are being developed , such as tagging one bird and tracking it back to establish where other members of the flock roost . Another technique is to analyse the DNA of Australian common starling populations to track where the migration from eastern to western Australia is occurring so that better preventive strategies can be used . By 2009 , only 300 common starlings were left in Western Australia , and the state committed a further A $ 400 @,@ 000 in that year to continue the eradication programme . \\n',\n",
       " ' In the United States , common starlings are exempt from the Migratory Bird Treaty Act , which prohibits the taking or killing of migratory birds . No permit is required to remove nests and eggs or kill juveniles or adults . Research was undertaken in 1966 to identify a suitable avicide that would both kill common starlings and would readily be eaten by them . It also needed to be of low toxicity to mammals and not likely to cause the death of pets that ate dead birds . The chemical that best fitted these criteria was DRC @-@ 1339 , now marketed as Starlicide . In 2008 , the United States government poisoned , shot or trapped 1 @.@ 7 million birds , the largest number of any nuisance species to be destroyed . In 2005 , the population in the United States was estimated at 140 million birds , around 45 % of the global total of 310 million . \\n',\n",
       " '',\n",
       " ' = = = In science and culture = = = \\n',\n",
       " '',\n",
       " ' Common starlings may be kept as pets or as laboratory animals . Austrian ethologist Konrad Lorenz wrote of them in his book King Solomon \\'s Ring as \" the poor man \\'s dog \" and \" something to love \" , because nestlings are easily obtained from the wild and after careful hand rearing they are straightforward to look after . They adapt well to captivity , and thrive on a diet of standard bird feed and mealworms . Several birds may be kept in the same cage , and their inquisitiveness makes them easy to train or study . The only disadvantages are their messy and indiscriminate defecation habits and the need to take precautions against diseases that may be transmitted to humans . As a laboratory bird , the common starling is second in numbers only to the domestic pigeon . \\n',\n",
       " ' The common starling \\'s gift for mimicry has long been recognised . In the medieval Welsh Mabinogion , Branwen tamed a common starling , \" taught it words \" , and sent it across the Irish Sea with a message to her brothers , Bran and Manawydan , who then sailed from Wales to Ireland to rescue her . Pliny the Elder claimed that these birds could be taught to speak whole sentences in Latin and Greek , and in Henry IV , William Shakespeare had Hotspur declare \" The king forbade my tongue to speak of Mortimer . But I will find him when he is asleep , and in his ear I \\'ll holler \\' Mortimer ! \\' Nay I \\'ll have a starling shall be taught to speak nothing but Mortimer , and give it to him to keep his anger still in motion . \" \\n',\n",
       " \" Mozart had a pet common starling which could sing part of his Piano Concerto in G Major ( KV . 453 ) . He had bought it from a shop after hearing it sing a phrase from a work he wrote six weeks previously , which had not yet been performed in public . He became very attached to the bird and arranged an elaborate funeral for it when it died three years later . It has been suggested that his A Musical Joke ( K. 522 ) might be written in the comical , inconsequential style of a starling 's vocalisation . Other people who have owned common starlings report how adept they are at picking up phrases and expressions . The words have no meaning for the starling , so they often mix them up or use them on what to humans are inappropriate occasions in their songs . Their ability at mimicry is so great that strangers have looked in vain for the human they think they have just heard speak . \\n\",\n",
       " ' Common starlings are trapped for food in some Mediterranean countries . The meat is tough and of low quality , so it is casseroled or made into pâté . One recipe said it should be stewed \" until tender , however long that may be \" . Even when correctly prepared , it may still be seen as an acquired taste . \\n',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.count_tokens(passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4486, 4638, 3913, 832]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_token_counts = list(map(lambda p: gpt.count_tokens(p), passages))\n",
    "passage_token_counts[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage token counts\n",
      "\n",
      "MEAN: 3875.3465818759937\n",
      "STD: 3161.958046450358\n",
      "MIN: 10\n",
      "MAX: 20498\n"
     ]
    }
   ],
   "source": [
    "print(\"Passage token counts\\n\")\n",
    "print(f\"MEAN: {np.mean(passage_token_counts)}\")\n",
    "print(f\"STD: {np.std(passage_token_counts)}\")\n",
    "print(f\"MIN: {np.min(passage_token_counts)}\")\n",
    "print(f\"MAX: {np.max(passage_token_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 / 629 passages greater than limit\n"
     ]
    }
   ],
   "source": [
    "# let's limit our token usage\n",
    "\n",
    "limit_n_tokens = 5000\n",
    "\n",
    "print(f\"{len([n for n in passage_token_counts if n > limit_n_tokens])} / {len(passages)} passages greater than limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest passage after trim is 4997 tokens\n"
     ]
    }
   ],
   "source": [
    "# elimite passages greater than our model's max token limit\n",
    "\n",
    "valid_idx = [i for i,v in enumerate(passage_token_counts) if v <= limit_n_tokens]\n",
    "valid_passages = [v for i,v in enumerate(passages) if i in valid_idx]\n",
    "\n",
    "# double check these passages are below the limit\n",
    "print(f\"largest passage after trim is {np.max(list(map(lambda p: gpt.count_tokens(p), valid_passages)))} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we should test to see if GPT is already able to answer questions about these passages from its internal knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" = You Only Live Twice ( film ) = \\n\\n\\n You Only Live Twice ( 1967 ) is the fifth spy film in the James Bond series , and the fifth to star Sean Connery as the fictional MI6 agent James Bond . The film 's screenplay was written by Roald Dahl , and loosely based on Ian Fleming 's 1964 novel of the same name . It is the first James Bond film to discard most of Fleming 's plot , using only a few characters and locations from the book as the background for an entirely new story . \\n\\n In the film , Bond\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_passages[112][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fifth film in the James Bond series is \"You Only Live Twice,\" released in 1967 and starring Sean Connery as James Bond.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(\"What is the fifth film in the James Bond series?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Trials and Tribble @-@ ations = \\n\\n\\n \" Trials and Tribble @-@ ations \" is the 104th episode of the American science fiction television series Star Trek : Deep Space Nine , the sixth episode of the fifth season . It was written as a tribute to the original series of Star Trek , in the 30th anniversary year of the show ; sister series Voyager produced a similar episode , \" Flashback \" . The idea for the episode was suggested by René Echevarria , and Ronald D. Moore suggested the link to \" The Tr'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_passages[152][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The episode \"Trials and Tribble-ations\" is from the TV show Star Trek: Deep Space Nine. It is the 6th episode of the 5th season.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(\"What show is the episode 'Trials and Tribble' from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT's internal knowledge base already has information about this dataset. Let's manipulate some of the information in our dataset so we can properly test our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_passages[112] = valid_passages[112].replace(\"You Only Live Twice\", \"No, YOLO\")\n",
    "valid_passages[152] = valid_passages[152].replace(\"Star Trek\", \"I'm More of a Star Wars Fan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df = pd.DataFrame()\n",
    "write_df[\"text\"] = valid_passages\n",
    "write_df.to_csv(\"passages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vectorstore for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create our database of documents to retrieve from. We could do this by embedding each document, storing the embeddings and text in a file and perform cosine similarity on the embeddings to find top matches against a query. However, it's better to use a vectorstore for this, as they're optimized for this task and very fast. We'll use FAISS through the langchain library for our vectorstore, but we can swap this out for another one if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import random\n",
    "\n",
    "def load_csv_file(file_path, shuffle=True, seed=None):\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path, encoding=\"utf-8\", csv_args={\"delimiter\": \",\"}\n",
    "    )\n",
    "    csv_data = loader.load()\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(csv_data)\n",
    "\n",
    "    return csv_data\n",
    "\n",
    "# passages_csv = load_csv_file(\"passages.csv\", seed=1)\n",
    "# passages_csv[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class LangchainVectorstore:\n",
    "    def __init__(self, embedding_type, processed_csv_path, verbose_info=True):\n",
    "        self.data = load_csv_file(processed_csv_path)\n",
    "        self.embedding_type = embedding_type\n",
    "        self.verbose = verbose_info\n",
    "        self.vectorstore = None # invoke create/load_local_vectorstore() to set\n",
    "        self.retriever = None # invoke create_retriever to set\n",
    "\n",
    "        if self.verbose:\n",
    "            logging.info(\"Vectorstore and retriever must be set using the class methods.\")\n",
    "\n",
    "    def chunk_data(self, chunk_size: int = 2048, chunk_overlap: int = 50):\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "                    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "                )\n",
    "        self.data = text_splitter.split_documents(self.data)\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Data chunked to size {chunk_size}\")\n",
    "\n",
    "    def create_local_vectorstore(self, save_path):\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            overwrite_saved = input(f\"Vectorstore already found at {save_path}; overwrite? [y/n]: \").lower()\n",
    "            if overwrite_saved not in [\"y\", \"n\"]:\n",
    "                raise ValueError(\"Invalid input; try again with 'y' for yes or 'n' for no.\")\n",
    "            \n",
    "            if overwrite_saved == \"n\":\n",
    "                if self.verbose:\n",
    "                    logging.info(\"Keeping saved vectorstore; aborting... \")\n",
    "                return None # break out of function\n",
    "\n",
    "        logging.info(f\"Creating a new local vectorstore at: {save_path}\")\n",
    "        try:\n",
    "            # no built in progress bar from their API; using this workaround shared at: https://stackoverflow.com/questions/77836174/how-can-i-add-a-progress-bar-status-when-creating-a-vector-store-with-langchain\n",
    "            with tqdm(total=len(self.data), desc=\"Processing documents\") as progress_bar:\n",
    "                for d in self.data:\n",
    "                    if self.vectorstore:\n",
    "                        self.vectorstore.add_documents([d])\n",
    "                    else: # init \n",
    "                        self.vectorstore = FAISS.from_documents([d], self.embedding_type)\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "            #self.vectorstore = FAISS.from_documents(self.data, self.embedding_type)\n",
    "            # above function is equivalent to embedding each piece of text, zipping text and embeddings as pairs, and creating index from these pairs\n",
    "            self.vectorstore.save_local(save_path)\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Vectorstore successfully set and saved to {save_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to create vectorstore: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            return None # not needed, but including a final return since we used one for a conditional abort\n",
    "            \n",
    "\n",
    "    def load_local_vectorstore(self, load_path):\n",
    "        if not os.path.exists(load_path):\n",
    "            raise ValueError(f\"Failed to find a saved vectorstore at {load_path}; please ensure save_path points to correct location.\")\n",
    "\n",
    "        try:\n",
    "            self.vectorstore = FAISS.load_local(load_path, self.embedding_type, allow_dangerous_deserialization=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load vectorstore: {e}\")\n",
    "\n",
    "    def create_retriever(self, search_type: str = \"similarity\", search_kwargs: dict = {}):\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore not set; create or load a vectorstore using class method first.\")\n",
    "\n",
    "        search_types = [\"similarity\", \"mmr\", \"similarity_score_threshold\"]\n",
    "        if search_type not in search_types:\n",
    "            raise ValueError(f\"Invalid arg for search_type; valid args include: {search_types}\")\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs=search_kwargs,\n",
    "        )\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Retriever successfully set\")\n",
    "\n",
    "    def retrieve_conext(self, query: str):\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Retriver not set; create a retriever using class method first\")\n",
    "        \n",
    "        retrieved_docs = self.retriever.get_relevant_documents(query)\n",
    "\n",
    "        return [retrieved_docs[i].page_content for i in range(len(retrieved_docs))]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a few methods for this vectorstore class, which includes:\n",
    " - creating the vectorstore and saving locally\n",
    " - loading the vectorstore locally, if already saved\n",
    " - creating the retriever to perform a similarity search over the db \n",
    " - using the retriever to return top documents related to a query\n",
    " - chunking the data to a specified token size \n",
    "\n",
    "We filtered out passaged larger than our desired token limit before, so we don't need to chunk now, but having the functionality may be useful in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:30:48,120 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2024-03-26 23:30:48,249 - INFO - Use pytorch device: cpu\n",
      "2024-03-26 23:30:48,279 - INFO - Vectorstore and retriever must be set using the class methods.\n"
     ]
    }
   ],
   "source": [
    "vs = LangchainVectorstore(\n",
    "    embedding_type = HuggingFaceEmbeddings(),\n",
    "    processed_csv_path = \"passages.csv\",\n",
    "    verbose_info = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:30:53,970 - INFO - Creating a new local vectorstore at: document_store/faiss_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/460 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:30:54,252 - INFO - Loading faiss.\n",
      "2024-03-26 23:30:54,265 - INFO - Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 460/460 [01:21<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:32:15,140 - INFO - Vectorstore successfully set and saved to document_store/faiss_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#vs.chunk_data()\n",
    "vs.create_local_vectorstore(save_path=\"document_store/faiss_index\")\n",
    "#vs.load_local_vectorstore(load_path=\"document_store/faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create the retriever, we can specify can how many documents to return with the \"k\" search kwarg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:32:24,588 - INFO - Retriever successfully set\n"
     ]
    }
   ],
   "source": [
    "vs.create_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our RAG search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 112\n",
      "text: = No, YOLO ( film ) = \n",
      "\n",
      "\n",
      " No, YOLO ( 1967 ) is the fifth spy film in the James Bond series , and the fifth to star Sean Connery as the fictional MI6 agent James Bond . The film 's screenplay was written by Roald Dahl , and loosely based on Ian Fleming 's 1964 novel of the same name . It is the first James Bond film to discard most of Fleming 's plot , using only a few characters and locations from the book as the background for an entirely new story . \n",
      "\n",
      " In the film , Bond is dispatched to Japan after American and Soviet manned spacecraft disappear mysteriously in orbit . With each nation blaming the other amidst the Cold War , Bond travels secretly to a remote Japanese island in order to find the perpetrators and comes face to face with Ernst Stavro Blofeld , the head of SPECTRE . The film reveals the appearance of Blofeld , who was previously a partially unseen character . SPECTRE is extorting the government of an unnamed Asian power , implied to be the People 's Republic\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the fifth film in the James Bond series?\"\n",
    "\n",
    "# use similarity search on vectorstore\n",
    "top_context = vs.retrieve_conext(query)\n",
    "print(top_context[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, looks like our retrieval is working as expected and returned the passage we manipulated. Before we perform a RAG query, we need to consider the LLM's token limit for a single prompt; we may need to trim some of the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context = \"\\n\\n\".join(top_context)\n",
    "\n",
    "prompt = all_context + \"\\n\\nBased on the above context, answer the follow question:\\n\" + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18593"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16135"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.max_prompt_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4486\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア ... ustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \n",
      "\n",
      "\n",
      "\n",
      "After: 1009\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア ... signed weapon . Changing class does not greatly affect the stats gained while in a previous class .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def truncate_text(text, gpt, token_limit):\n",
    "    token_count = 0\n",
    "    truncated_text = \"\"\n",
    "    try:\n",
    "        for line in text.split(\".\"):\n",
    "            if line.strip() in [\"\"]:\n",
    "                continue\n",
    "\n",
    "            line = line + \".\"\n",
    "            token_count += gpt.count_tokens(line)\n",
    "            if token_count >= token_limit:\n",
    "                break\n",
    "\n",
    "            truncated_text += line\n",
    "\n",
    "        truncated_text += \"\\n\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to truncate text: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return truncated_text\n",
    "\n",
    "# test function\n",
    "\n",
    "print(f\"Before: {gpt.count_tokens(passages[0])}\")\n",
    "print(f\"{passage[:100]} ... {passage[-100:]}\")\n",
    "\n",
    "truncated_passage = truncate_text(passages[0], gpt, token_limit=1024)\n",
    "print(f\"After: {gpt.count_tokens(truncated_passage)}\")\n",
    "print(f\"{truncated_passage[:100]} ... {truncated_passage[-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16113"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can build this check into the gpt class to truncate any prompts over the limit before sending\n",
    "\n",
    "all_context = \"\\n\\n\".join(top_context)\n",
    "query = \"\\n\\nBased on the above context, answer the follow question:\\n\" + query\n",
    "\n",
    "buffer_token_space = gpt.count_tokens(query)\n",
    "token_limit = gpt.max_prompt_tokens - buffer_token_space\n",
    "\n",
    "if gpt.count_tokens(all_context) > token_limit:\n",
    "    prompt = truncate_text(all_context, gpt, token_limit) + query\n",
    "else:\n",
    "    prompt = all_context + query\n",
    "    \n",
    "gpt.count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to adjust the system prompt to ensure GPT only uses the provided document information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.system_role = \"You will answer user queries based on the context provided. You will limit your answers ONLY to the information provided and will NOT provide any external information. If the information needed to answer the query is not present in the input, or no additional context is provided, you will reply with 'I can't answer that based on the provided documents'.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:32:45,482 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The fifth film in the James Bond series is 'No, YOLO'.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:32:56,200 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The episode 'Trials and Tribble @-@ ations' is from the American science fiction television series 'I'm More of a Star Wars Fan: Deep Space Nine'.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What American science fiction television series is the episode 'Trials and Tribble' from?\"\n",
    "\n",
    "top_context = vs.retrieve_conext(query)\n",
    "all_context = \"\\n\\n\".join(top_context)\n",
    "query = \"\\n\\nBased on the above context, answer the follow question:\\n\" + query\n",
    "\n",
    "buffer_token_space = gpt.count_tokens(query)\n",
    "token_limit = gpt.max_prompt_tokens - buffer_token_space\n",
    "\n",
    "if gpt.count_tokens(all_context) > token_limit:\n",
    "    prompt = truncate_text(all_context, gpt, token_limit) + query\n",
    "else:\n",
    "    prompt = all_context + query\n",
    "\n",
    "gpt.post_prompt(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-26 23:33:10,927 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can't answer that based on the provided documents.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check response when not providing any of our context\n",
    "\n",
    "gpt.post_prompt(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Looks like our RAG system is functioning properly and we've been able to limit GPT to the documents we manipulated. We now have all the pieces we need for our program and can wrap all of this up into a project code base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_WORKSPACE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
