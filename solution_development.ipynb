{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Document QA - Code Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the workspace for creating the code used in this project, which can help a user understand certain components of the program and verify their functionality. I've gone through and added markdown and comments to explain the development thought process, but I won't be adding docstrings or worrying about PEP8 standards in these functions until moving the code to modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object to handle our communications with the LLM. We'll use GPT here - you'll need to have an access token saved to a txt file in order to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define paths to use throughout this notebook\n",
    "\n",
    "ACCESS_TOKEN_PATH = os.path.pardir + \"/api_keys/openai.key\"\n",
    "SAVE_PATH = os.getcwd() + \"/document_store/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# check key path is valid\n",
    "assert os.path.exists(ACCESS_TOKEN_PATH), \"Access token key not found\"\n",
    "\n",
    "# create save dir if new run\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.mkdir(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import logging\n",
    "\n",
    "# class for GPT communication; won't worry about documentation until moving code to modules\n",
    "\n",
    "class GPTCommunicator():\n",
    "\n",
    "    def __init__(\n",
    "            self, api_key_path: str, model_name: str = \"gpt-3.5-turbo\",\n",
    "        ):\n",
    "\n",
    "        # init client with api key file\n",
    "        with open(api_key_path) as f:\n",
    "            self.client = OpenAI(api_key=f.readline().strip())\n",
    "        \n",
    "        # context window limits; found at https://platform.openai.com/docs/models\n",
    "        model_max_tokens = { \n",
    "            #\"gpt-3.5-turbo-instruct\": 4096,\n",
    "            \"gpt-3.5-turbo\": 16385,\n",
    "            \"gpt-4\": 8192,\n",
    "            \"gpt-4-32k\": 32768,\n",
    "        }\n",
    "\n",
    "        # check for valid model name input\n",
    "        if model_name not in model_max_tokens.keys():\n",
    "            raise ValueError(f\"Invalid model name; valid args include: {model_max_tokens.keys()}\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # set model attributes\n",
    "        self.max_prompt_tokens = model_max_tokens[model_name] -  250 # buffer for response tokens\n",
    "        self.system_role = \"You are a helpful AI assistant.\"\n",
    "        self.total_tokens_used = 0\n",
    "        \n",
    "    def post_prompt(self, text):\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model = self.model_name,\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": str(self.system_role)},\n",
    "                    {\"role\": \"user\", \"content\": str(text)}\n",
    "                ]\n",
    "            )\n",
    "            self.last_response = response\n",
    "            self.total_tokens_used += int(response.usage.total_tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to post prompt: {e}\")\n",
    "            return None\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        encoding = tiktoken.encoding_for_model(self.model_name)\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "\n",
    "        return num_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTCommunicator(ACCESS_TOKEN_PATH)\n",
    "\n",
    "# test communication\n",
    "response = gpt.post_prompt(\"Hello\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.total_tokens_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=9, prompt_tokens=19, total_tokens=28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.last_response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.count_tokens(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Europe is a continent located in the Northern Hemisphere, bordered by the Arctic Ocean to the north, the Atlantic Ocean to the west, the Mediterranean Sea to the south, and Asia to the east. It is known for its rich history, diverse cultures, and stunning landscapes, including mountains, forests, rivers, and coastlines. Europe is home to numerous iconic landmarks, such as the Eiffel Tower in Paris, the Colosseum in Rome, and the Acropolis in Athens.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(\"Describe Europe in 3 sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.total_tokens_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=98, prompt_tokens=25, total_tokens=123)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.last_response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communication and token tracking is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a set of text documents to develop our RAG solution. We'll use the wikitext dataset for this, which contains passages from wikipedia articles that we can use as our documents to QA over. \n",
    "\n",
    "There's a good chance GPT and other LLMs already have knowledge of the info in this dataset, so we may have to manipulate some of the information if we want to properly test our RAG system (more on this below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/anaconda3/envs/LLM_WORKSPACE/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
       " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       " '',\n",
       " ' = = Gameplay = = \\n',\n",
       " '',\n",
       " \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext[\"train\"][\"text\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "The text is split into a list of strings. We want to combine the strings to form full articles as individual documents. Based on visual inspection, it seems a delimiter \" = \" on both sides of a text is used to mark titles, \" = = \" for headers and \" = = = \" for sub-headers. We can group the text based on the start of a new title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# confirm \" = = = \" is the lowest category\n",
    "\n",
    "np.max([int(t.count(\" = \") / 2) for t in wikitext[\"train\"][\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to classify string as title/header/content based on the delimiters we saw above\n",
    "\n",
    "def classify_string_type(text):\n",
    "    \n",
    "    if text == '':\n",
    "        return \"empty\"\n",
    "    \n",
    "    title_delimiter = \" = \"\n",
    "    header_delimiter = \" = = \"\n",
    "    subheader_delimiter = \" = = = \"\n",
    "\n",
    "    def check_by_delimiter(t, delimiter):\n",
    "        # when split by the right delimiter, text will be in the form: ['', text, '\\n']\n",
    "        t_split = t.split(delimiter)\n",
    "\n",
    "        # for titles and headers, we can expect split == 3 and split[-1] == \\n\n",
    "        if len(t_split) == 3 and t_split[-1] == '\\n':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    if check_by_delimiter(text, subheader_delimiter):\n",
    "        return \"subheader\"\n",
    "    \n",
    "    elif check_by_delimiter(text, header_delimiter):\n",
    "        return \"header\"\n",
    "    \n",
    "    elif check_by_delimiter(text, title_delimiter):\n",
    "        return \"title\"\n",
    "    \n",
    "    else:\n",
    "        return \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = wikitext[\"train\"][\"text\"][1]\n",
    "print(text)\n",
    "classify_string_type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text text_type\n",
       "0                                                        empty\n",
       "1                     = Valkyria Chronicles III = \\n     title\n",
       "2                                                        empty\n",
       "3   Senjō no Valkyria 3 : Unrecorded Chronicles (...   content\n",
       "4   The game began development in 2010 , carrying...   content"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_list = wikitext[\"train\"][\"text\"]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = text_list\n",
    "df[\"text_type\"] = list(map(lambda t: classify_string_type(t), text_list))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_type\n",
       "content      17870\n",
       "empty        12951\n",
       "header        2922\n",
       "subheader     2346\n",
       "title          629\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It met with positive sales in Japan , and was...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>= = Gameplay = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As with previous Valkyira Chronicles games , ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The game 's battle system , the BliTZ system ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Troops are divided into five classes : Scouts...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>= = Plot = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The game takes place during the Second Europa...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>As the Nameless officially do not exist , the...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Partly due to these events , and partly due t...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>= = Development = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Concept work for Valkyria Chronicles III bega...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The majority of material created for previous...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>= = = Music = = = \\n</td>\n",
       "      <td>subheader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The music was composed by Hitoshi Sakimoto , ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>= = = Release = = = \\n</td>\n",
       "      <td>subheader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>In September 2010 , a teaser website was reve...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Unlike its two predecessors , Valkyria Chroni...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>= = Reception = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>On its day of release in Japan , Valkyria Chr...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Famitsu enjoyed the story , and were particul...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>PlayStation Official Magazine - UK praised th...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>In a preview of the TGS demo , Ryan Geddes of...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>= = Legacy = = \\n</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Kurt and Riela were featured in the Nintendo ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>= = = Adaptations = = = \\n</td>\n",
       "      <td>subheader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Valkyria Chronicles 3 was adapted into a two ...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>The anime 's title was inspired by the princi...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Two manga adaptations were produced , followi...</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal = \\n</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  text_type\n",
       "1                      = Valkyria Chronicles III = \\n      title\n",
       "2                                                          empty\n",
       "3    Senjō no Valkyria 3 : Unrecorded Chronicles (...    content\n",
       "4    The game began development in 2010 , carrying...    content\n",
       "5    It met with positive sales in Japan , and was...    content\n",
       "6                                                          empty\n",
       "7                                 = = Gameplay = = \\n     header\n",
       "8                                                          empty\n",
       "9    As with previous Valkyira Chronicles games , ...    content\n",
       "10   The game 's battle system , the BliTZ system ...    content\n",
       "11   Troops are divided into five classes : Scouts...    content\n",
       "12                                                         empty\n",
       "13                                    = = Plot = = \\n     header\n",
       "14                                                         empty\n",
       "15   The game takes place during the Second Europa...    content\n",
       "16   As the Nameless officially do not exist , the...    content\n",
       "17   Partly due to these events , and partly due t...    content\n",
       "18                                                         empty\n",
       "19                             = = Development = = \\n     header\n",
       "20                                                         empty\n",
       "21   Concept work for Valkyria Chronicles III bega...    content\n",
       "22   The majority of material created for previous...    content\n",
       "23                                                         empty\n",
       "24                               = = = Music = = = \\n  subheader\n",
       "25                                                         empty\n",
       "26   The music was composed by Hitoshi Sakimoto , ...    content\n",
       "27                                                         empty\n",
       "28                             = = = Release = = = \\n  subheader\n",
       "29                                                         empty\n",
       "30   In September 2010 , a teaser website was reve...    content\n",
       "31   Unlike its two predecessors , Valkyria Chroni...    content\n",
       "32                                                         empty\n",
       "33                               = = Reception = = \\n     header\n",
       "34                                                         empty\n",
       "35   On its day of release in Japan , Valkyria Chr...    content\n",
       "36   Famitsu enjoyed the story , and were particul...    content\n",
       "37   PlayStation Official Magazine - UK praised th...    content\n",
       "38   In a preview of the TGS demo , Ryan Geddes of...    content\n",
       "39                                                         empty\n",
       "40                                  = = Legacy = = \\n     header\n",
       "41                                                         empty\n",
       "42   Kurt and Riela were featured in the Nintendo ...    content\n",
       "43                                                         empty\n",
       "44                         = = = Adaptations = = = \\n  subheader\n",
       "45                                                         empty\n",
       "46   Valkyria Chronicles 3 was adapted into a two ...    content\n",
       "47   The anime 's title was inspired by the princi...    content\n",
       "48   Two manga adaptations were produced , followi...    content\n",
       "49                                                         empty\n",
       "50                                                         empty\n",
       "51   = Tower Building of the Little Rock Arsenal = \\n      title"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_idx = df.index[df['text_type']==\"title\"].tolist()\n",
    "df.iloc[title_idx[0]:title_idx[1]+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\n",
    "\n",
    "Looks like our classifying function is working properly; we can now split the list at the index of each title and group the lines into full passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "\n",
      "\n",
      " = = Gameplay = = \n",
      "\n",
      "\n",
      " As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements r\n"
     ]
    }
   ],
   "source": [
    "passage = \"\\n\".join(df.iloc[title_idx[0]:title_idx[1]][\"text\"])\n",
    "print(passage[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ation 4 that forms the beginning of a new series within the Valkyria franchise . \n",
      "\n",
      "\n",
      " = = = Adaptations = = = \n",
      "\n",
      "\n",
      " Valkyria Chronicles 3 was adapted into a two @-@ episode original video animation series in the same year of its release . Titled Senjō no Valkyria 3 : Taga Tame no Jūsō ( 戦場のヴァルキュリア３ 誰がための銃瘡 , lit . Valkyria of the Battlefield 3 : The Wound Taken for Someone 's Sake ) , it was originally released through PlayStation Network and Qriocity between April and May 2011 . The initially @-@ planned release and availability period needed to be extended due to a stoppage to PSN during the early summer of that year . It later released for DVD on June 29 and August 31 , 2011 , with separate \" Black \" and \" Blue \" editions being available for purchase . The anime is set during the latter half of Valkyria Chronicles III , detailing a mission by the Nameless against their Imperial rivals Calamity Raven . The anime was first announced in November 2010 . It was developed by A @-@ 1 Pictures , produced by Shinji Motoyama , directed by Nobuhiro Kondō , and written by Hiroshi Ōnogi . Sakimoto 's music for the game was used in the anime . \n",
      "\n",
      " The anime 's title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of others . A subtitle attached to the project during development was \" The Road to Kubinka \" , which referenced the Kubinka Tank Museum in Moscow . The game 's main theme was how the characters regained their sense of self when stripped of their names and identities , along with general themes focused on war and its consequences . While making the anime , the production team were told by Sega to make it as realistic as possible , with the consequence that the team did extensive research into aspects such as what happened when vehicles like tanks were overturned or damaged . Due to it being along the same timeline as the original game and its television anime adaptation , the cast of Valkyria Chronicles could make appearances , which pleased the team . The opening theme , \" Akari ( Light ) -Tomoshibi- \" ( 灯 @-@ TOMOSHIBI- ) , was sung by Japanese singer Faylan . The ending theme , \" Someday the Flowers of Light Will Bloom \" ( いつか咲く光の花 , Itsuka Saku Hikari no Hana ) , was sung by Minami Kuribayashi . Both songs ' lyrics were written by their respective artists . \n",
      "\n",
      " Two manga adaptations were produced , following each of the game 's main female protagonists Imca and Riela . They were Senjō no Valkyria 3 : Namo naki Chikai no Hana ( 戦場のヴァルキュリア3 名もなき誓いの花 , lit . Valkyria of the Battlefield 3 : The Flower of the Nameless Oath ) , illustrated by Naoyuki Fujisawa and eventually released in two volumes after being serialized in Dengeki Maoh between 2011 and 2012 ; and Senjō no Valkyria 3 : -Akaki Unmei no Ikusa Otome- ( 戦場のヴァルキュリア3 -赤き運命の戦乙女- , lit . Valkyria of the Battlefield 3 -The Valkyrie of the Crimson Fate ) , illustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(passage[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform list of single text lines into list of passages. These passages will be saved and used as documents for our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア ... ustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def transform_into_passages(text_list):\n",
    "\n",
    "    text_type = list(map(lambda t: classify_string_type(t), text_list))\n",
    "\n",
    "    type_counts = Counter(text_type) #dict storing counts of titles, headers, etc.\n",
    "\n",
    "    title_idx = np.array([i for i,v in enumerate(text_type) if v == \"title\"])\n",
    "    title_idx = np.append(title_idx, len(text_list)) #append for last passage\n",
    "    title_idx_pairs = np.column_stack((title_idx[:-1], title_idx[1:]))\n",
    "\n",
    "    passages = []\n",
    "\n",
    "    for idx_pair in title_idx_pairs:\n",
    "        start_i, end_i = idx_pair[0], idx_pair[1]\n",
    "        passage = \"\\n\".join(text_list[start_i:end_i])\n",
    "        passages.append(passage)\n",
    "\n",
    "    assert len(passages) == type_counts[\"title\"], \"Passage count should match number of titles\"\n",
    "\n",
    "    return passages\n",
    "\n",
    "passages = transform_into_passages(text_list)\n",
    "print(f\"{passages[0][:100]} ... {passages[0][-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Tower Building of the Little Rock Arsenal = \n",
      "\n",
      "\n",
      " The Tower Building of the Little Rock Arsenal , a ... emen and servicewomen of the United States and commemorate the birthplace of Douglas MacArthur . \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{passages[1][:100]} ... {passages[1][-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y , which had not yet been performed in public . He became very attached to the bird and arranged an elaborate funeral for it when it died three years later . It has been suggested that his A Musical Joke ( K. 522 ) might be written in the comical , inconsequential style of a starling 's vocalisation . Other people who have owned common starlings report how adept they are at picking up phrases and expressions . The words have no meaning for the starling , so they often mix them up or use them on what to humans are inappropriate occasions in their songs . Their ability at mimicry is so great that strangers have looked in vain for the human they think they have just heard speak . \n",
      "\n",
      " Common starlings are trapped for food in some Mediterranean countries . The meat is tough and of low quality , so it is casseroled or made into pâté . One recipe said it should be stewed \" until tender , however long that may be \" . Even when correctly prepared , it may still be seen as an acquired taste . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verify against text_list\n",
    "\n",
    "print(passages[-1][-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Western Australia banned the import of common starlings in 1895 . New flocks arriving from the east are routinely shot , while the less cautious juveniles are trapped and netted . New methods are being developed , such as tagging one bird and tracking it back to establish where other members of the flock roost . Another technique is to analyse the DNA of Australian common starling populations to track where the migration from eastern to western Australia is occurring so that better preventive strategies can be used . By 2009 , only 300 common starlings were left in Western Australia , and the state committed a further A $ 400 @,@ 000 in that year to continue the eradication programme . \\n',\n",
       " ' In the United States , common starlings are exempt from the Migratory Bird Treaty Act , which prohibits the taking or killing of migratory birds . No permit is required to remove nests and eggs or kill juveniles or adults . Research was undertaken in 1966 to identify a suitable avicide that would both kill common starlings and would readily be eaten by them . It also needed to be of low toxicity to mammals and not likely to cause the death of pets that ate dead birds . The chemical that best fitted these criteria was DRC @-@ 1339 , now marketed as Starlicide . In 2008 , the United States government poisoned , shot or trapped 1 @.@ 7 million birds , the largest number of any nuisance species to be destroyed . In 2005 , the population in the United States was estimated at 140 million birds , around 45 % of the global total of 310 million . \\n',\n",
       " '',\n",
       " ' = = = In science and culture = = = \\n',\n",
       " '',\n",
       " ' Common starlings may be kept as pets or as laboratory animals . Austrian ethologist Konrad Lorenz wrote of them in his book King Solomon \\'s Ring as \" the poor man \\'s dog \" and \" something to love \" , because nestlings are easily obtained from the wild and after careful hand rearing they are straightforward to look after . They adapt well to captivity , and thrive on a diet of standard bird feed and mealworms . Several birds may be kept in the same cage , and their inquisitiveness makes them easy to train or study . The only disadvantages are their messy and indiscriminate defecation habits and the need to take precautions against diseases that may be transmitted to humans . As a laboratory bird , the common starling is second in numbers only to the domestic pigeon . \\n',\n",
       " ' The common starling \\'s gift for mimicry has long been recognised . In the medieval Welsh Mabinogion , Branwen tamed a common starling , \" taught it words \" , and sent it across the Irish Sea with a message to her brothers , Bran and Manawydan , who then sailed from Wales to Ireland to rescue her . Pliny the Elder claimed that these birds could be taught to speak whole sentences in Latin and Greek , and in Henry IV , William Shakespeare had Hotspur declare \" The king forbade my tongue to speak of Mortimer . But I will find him when he is asleep , and in his ear I \\'ll holler \\' Mortimer ! \\' Nay I \\'ll have a starling shall be taught to speak nothing but Mortimer , and give it to him to keep his anger still in motion . \" \\n',\n",
       " \" Mozart had a pet common starling which could sing part of his Piano Concerto in G Major ( KV . 453 ) . He had bought it from a shop after hearing it sing a phrase from a work he wrote six weeks previously , which had not yet been performed in public . He became very attached to the bird and arranged an elaborate funeral for it when it died three years later . It has been suggested that his A Musical Joke ( K. 522 ) might be written in the comical , inconsequential style of a starling 's vocalisation . Other people who have owned common starlings report how adept they are at picking up phrases and expressions . The words have no meaning for the starling , so they often mix them up or use them on what to humans are inappropriate occasions in their songs . Their ability at mimicry is so great that strangers have looked in vain for the human they think they have just heard speak . \\n\",\n",
       " ' Common starlings are trapped for food in some Mediterranean countries . The meat is tough and of low quality , so it is casseroled or made into pâté . One recipe said it should be stewed \" until tender , however long that may be \" . Even when correctly prepared , it may still be seen as an acquired taste . \\n',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "Now that we have our passages to be used as documents, we should inspect the token counts and consider filtering some to limit our token usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.count_tokens(passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4486, 4638, 3913, 832]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_token_counts = list(map(lambda p: gpt.count_tokens(p), passages))\n",
    "passage_token_counts[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage token counts\n",
      "\n",
      "MEAN: 3875.3465818759937\n",
      "STD: 3161.958046450358\n",
      "MIN: 10\n",
      "MAX: 20498\n"
     ]
    }
   ],
   "source": [
    "print(\"Passage token counts\\n\")\n",
    "print(f\"MEAN: {np.mean(passage_token_counts)}\")\n",
    "print(f\"STD: {np.std(passage_token_counts)}\")\n",
    "print(f\"MIN: {np.min(passage_token_counts)}\")\n",
    "print(f\"MAX: {np.max(passage_token_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 / 629 passages greater than limit\n"
     ]
    }
   ],
   "source": [
    "# let's limit our token usage for now\n",
    "\n",
    "limit_n_tokens = 5000\n",
    "\n",
    "print(f\"{len([n for n in passage_token_counts if n > limit_n_tokens])} / {len(passages)} passages greater than limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest passage after trim is 4997 tokens\n"
     ]
    }
   ],
   "source": [
    "# elimite passages greater than our limit\n",
    "\n",
    "valid_idx = [i for i,v in enumerate(passage_token_counts) if v <= limit_n_tokens]\n",
    "valid_passages = [v for i,v in enumerate(passages) if i in valid_idx]\n",
    "\n",
    "# double check these passages are below the limit\n",
    "print(f\"largest passage after trim is {np.max(list(map(lambda p: gpt.count_tokens(p), valid_passages)))} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "Before we continue, we should test to see if GPT is already able to answer questions about these passages from its internal knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" = You Only Live Twice ( film ) = \\n\\n\\n You Only Live Twice ( 1967 ) is the fifth spy film in the James Bond series , and the fifth to star Sean Connery as the fictional MI6 agent James Bond . The film 's screenplay was written by Roald Dahl , and loosely based on Ian Fleming 's 1964 novel of the same name . It is the first James Bond film to discard most of Fleming 's plot , using only a few characters and locations from the book as the background for an entirely new story . \\n\\n In the film , Bond\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_passages[112][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fifth film in the James Bond series is \"You Only Live Twice,\" released in 1967.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(\"What is the fifth film in the James Bond series?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Trials and Tribble @-@ ations = \\n\\n\\n \" Trials and Tribble @-@ ations \" is the 104th episode of the American science fiction television series Star Trek : Deep Space Nine , the sixth episode of the fifth season . It was written as a tribute to the original series of Star Trek , in the 30th anniversary year of the show ; sister series Voyager produced a similar episode , \" Flashback \" . The idea for the episode was suggested by René Echevarria , and Ronald D. Moore suggested the link to \" The Tr'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_passages[152][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The episode \"Trials and Tribble-ations\" is from the TV show \"Star Trek: Deep Space Nine.\" It is the 6th episode of the 5th season.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(\"What show is the episode 'Trials and Tribble' from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "GPT's internal knowledge base already has information about this dataset. Let's manipulate some of the information in our dataset so we can properly test our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_passages[112] = valid_passages[112].replace(\"You Only Live Twice\", \"No, YOLO\")\n",
    "valid_passages[152] = valid_passages[152].replace(\"Star Trek\", \"I'm More of a Star Wars Fan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df = pd.DataFrame()\n",
    "write_df[\"text\"] = valid_passages\n",
    "write_df.to_csv(SAVE_PATH+\"passages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vectorstore for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create our database of documents to retrieve from. We could do this by embedding each document, storing the embeddings and text in a file and perform cosine similarity on the embeddings to find top matches against a query. However, it's better to use a vectorstore for this, as they're optimized for this task and very fast. We'll use FAISS through the langchain library for our vectorstore, but we can swap this out for another one if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=': 289\\ntext: = Freakum Dress = \\n\\n\\n \" Freakum Dress \" is a song by American singer and songwriter Beyoncé from her second solo studio album B \\'Day ( 2006 ) . It was written by Beyoncé , Rich Harrison , and Makeba Riddick . \" Freakum Dress \" is similar to songs that Destiny \\'s Child used to record in the 1990s . The song is complete with whistles , cymbal dominated scatter rhythms and a beat , which is augmented by hi @-@ hats and plinking keyboard pulses . In the song , Beyoncé advises women who have partners with straying eyes to put on alluring dresses and grind on other guys in dance clubs , to regain their affections . \\n\\n \" Freakum Dress \" was generally well received by music critics who complimented Beyoncé \\'s vocals as well as the assertiveness with which she delivers the lyrics . Many of them also noted that the beat of song melds very well with the vocal arrangement and the instruments used . The music video for the song was directed by Ray Kay , with co @-@ direction from Beyoncé , for the B \\'Day Anthology Video Album ( 2007 ) . It features Beyoncé dancing with women of different ages , races , and sizes . Thirty metallic dresses were designed by Tina Knowles and were used in the production . Beyoncé explained that the main reason behind shooting a video for the song was to show what a \" freakum dress \" looks like . The song was part of the set lists during Beyoncé \\'s worldwide tours The Beyoncé Experience ( 2007 ) and I Am ... World Tour ( 2009 – 10 ) . Later , in 2012 , the song was performed during her revue Revel Presents : Beyoncé Live . \\n\\n\\n = = Recording and conception = = \\n\\n\\n \" Freakum Dress \" was conceived at Sony Music Studios , in New York City , when Beyoncé enlisted Harris to co @-@ produce for her album B \\'Day ( 2006 ) . She and Harrison had previously collaborated on her 2003 single \" Crazy in Love \" . She arranged for Harrison , Sean Garrett and Rodney Jerkins to be given individual rooms at the studio . In this way , Beyoncé fostered \" healthy competition \" between the producers by going into each of their rooms and commenting on the \" great beats \" the others were creating . Roger Friedman of Fox News Channel noted that \" Freakum Dress \" and \" Suga Mama \" ( 2006 ) , Harrison \\'s other contribution to B \\'Day \" fall short of originality but mimic the Chi Lites [ sic ] percussion section [ of \" Crazy in Love \" ] yet again \" , adding , \" Harrison is like the Indiana Jones of soul , constantly pulling out forgotten gems of the past for sampling [ ... ] You can \\'t help but think : Thank God someone wrote music in the past that can be repurposed now . \" Harrison wrote \" Freakum dress to demonstrate how a sassy sartorial item that can help recharge to a relationship \" , with Beyoncé and Makeba Riddick also contributing In an interview with USA Today , Beyoncé talked about the content \" Freakum Dress \" , stating that an outfit which reminds of the best moments in a couple \\'s life , is a necessity for every woman \\'s wardrobe . \\n\\n In June 2006 , Beyoncé invited Tamara Coniff of Billboard to a New York recording studio . There she premiered several songs from the album including \" Ring the Alarm \" ( 2006 ) and \" Freakum Dress \" , both were cited as possible second singles although in the end it was actually \" Ring the Alarm \" that became B \\'Day \\'s second single . Beyoncé told Coniff that \" Freakum Dress \" was one of her favorite songs ever . \\n\\n\\n = = Music and theme = = \\n\\n\\n According to the sheet music published at Musicnotes.com by Hal Leonard Corporation , \" Freakum Dress \" is a moderate R & B song pacing in common time , written in the key of F major . The verses alternate from the chords of F ♯ and C. The track also draws from the hip hop , funk , and dance @-@ pop genres . Mike Joseph of PopMatters observed that the song shows influences by 1970s funk music , and contains limited elements of 1980s go @-@ go . According to Phil Harrison of Timeout , \" Freakum Dress \" consists of a steady \" long crescendo , welding galloping beats and a steamrolling two @-@ note riff \" , accompanied by several genres of music , which he qualified as \" multi @-@ tracked \" . Spence D. of IGN Music noted that the song consists of frequent whistles as well as crashing cymbal dominated scatter rhythms and a beat which fits the \" powerful , loud , confident lines \" in which Beyoncé asks for the attention of her man , and urges women to have a beautiful dress to spice up their sexual life . \" Freakum Dress \" opens with a spoken introduction . Throughout the song , Beyoncé sings her lines in an assertive manner on melding shattering hi @-@ hats \" and plinking keyboard pulses . \\n\\n According to Joseph , \" Freakum Dress \" is thematically similar to \" Bills , Bills , Bills \" ( 1999 ) and \" Say My Name \" ( 2000 ) , from the Destiny \\'s Child era . Ann Powers of Los Angeles Times noted that \" Freakum Dress \" celebrates showing off . Jon Pareles of The New York Times viewed the concept of the song as not merely having a nice wardrobe to entice men , but it also serves as \" a means of self @-@ assertion . \" In the song , the female protagonist pulls out her best dress to remind her potentially wandering mate of what he is leaving at home . Jody Rosen of Entertainment Weekly added that Beyoncé also seemingly gives professional advice to women on how to hold a man \\'s attention in a long @-@ term relationship . She sings : \" I think I \\'m ready / Been locked up in the house way too long / It \\'s time to get it , [ be ] cause once again he \\'s out doing wrong [ ... ] Wear very skimpy clothes ... \" . Joseph commented that in the song , Beyoncé is capable of wearing anything to keep her man by her side rather than dumping him . Sarah Rodman of The Boston Globe added that after having skirted her best dress , Beyoncé eyes other guys in dance clubs to make her own man jealous , in the hope of regaining his attention but she also makes sure that he really pays when he does her wrong . Beyoncé later refers to her \" freakum dress \" in \" Jealous \" , a track from her fifth studio album Beyoncé ( 2013 ) . \\n\\n\\n = = Reception = = \\n\\n\\n The song received mostly positive reviews . Phil Harrison of Timeout called \" Freakum Dress \" a magnificent production thanks to its vocal arrangements and commented that its beat can \" drive the boys crazy . \" Brian Hiatt of Rolling Stone magazine wrote that even though \" Freakum Dress \" is less harmonically and melodically produced than \" Crazy In Love \" ( 2003 ) and songs from the Destiny \\'s Child era , it remains a good track due to its highly energetic beat . Jaime Gill of Yahoo ! Music called the track \" discordant \" and \" menacing \" while Jon Pareles of The New York Times called it \" overwrought \" . On a separate review , Jon Pareles said that the song will remain as one of Beyoncé most memorable tracks thanks to its streak of rage which is \" perfectly groomed but unmistakable \" . Bill Lamb of About.com chose \" Freakum Dress \" as one of the three best songs on the entire record , and called it a powerful , emotionally intensive and energetic track . Caroline Sullivan of The Guardian called the song a \" lighthearted crunk spree \" that reminds girls of the significance of having a nice dress in their wardrobe . \\n\\n Mike Joseph of PopMatters complimented the overall concept of the song but noted that the lyrics do not \" radiate \" enough warmth . Sal Cinquemani of Slant Magazine jokingly said that Beyoncé has added the term \" Freakum Dress \" \" to the pop lexicon . \" Elysa Gardner of USA Today said that \" self @-@ assurance is evident on a tune on B \\'Day called \\' Freakum Dress \\' \" while another review by the staff members of the same magazine complimented the songs sexual imagery stating : \" When Ms. Bootylicious [ Beyoncé ] sings of squeezing that jelly into a \\' Freakum Dress \\' , the imagination runs wilder than any video would . Darryl Sterdan , writing for the Canadian website Jam ! , complimented the song \\'s \" bashing beat and irresistible chorus \" . Andy Kellman of Allmusic described \" Freakum dress \" as a \" blaring and marching \" track . Calling \" Freakum Dress \" one of the best dance track that Beyoncé has ever sung , Norman Mayers of Prefix Magazine chose it as one of the standout songs of the album . While reviewing B \\'Day , Chuck Arnold of People magazine wrote , \" \\' ladies \\' anthem \\' Freakum Dress \\' finds Beyoncé working all her bootylicious powers over some slamming funk \" . \" Freakum Dress \" reached number twenty @-@ five on the US Bubbling Under Hot 100 Singles chart issue dated September 9 , 2007 . The same day , it also charted on the US Bubbling Under R & B / Hip @-@ Hop Singles at number sixteen . \\n\\n\\n = = Music video = = \\n\\n\\n\\n = = = Concept and filming = = = \\n\\n\\n The music video was co @-@ directed by Ray Kay and Beyoncé for the B \\'Day Anthology Video Album , which was released the same month : it was one of eight videos shot in two weeks for the album . The choreography was done by Danielle Polanco and Jonte \\' Moaning , who used a 1980 ’ s retro set . Beyoncé explained the concept of the video at MTV : \" It \\'s probably the most flamboyant video , and the metallic dresses are so beautiful , they added so much color . I had to do a video for this song . Everyone wanted to know what a \\' freakum dress \\' was , and you can \\'t really explain it , you have to see it . Everyone has their own version , so we had so many women — of different races , sizes , shapes , ages — because we all have those dresses we pull out when we need to shut it down . \" \\n\\n After two weeks of shooting , Beyoncé decided to call her mother Tina . The latter designed thirty dresses for the video , with eight of them for her daughter . Due to limited time , certain dress were sewed on the spot in approximately ten minutes each by taking fabric from one dress , making a slit in it , draping it and putting a belt on it . The glasses that Beyoncé wears in the video were borrowed from her make @-@ up artist , Francesca Tolot . The video was finished in about eighteen hours of filming and it features Ebony Haith from America \\'s Next Top Model , Cycle 1 . Throughout the video , Beyoncé can be seen fixing her hair in a neon mirror and is surrounded by neon @-@ constructed doors , catwalks and podiums . It premiered on BET \\'s 106 & Park and on American Music Channel , among others , before the release of the video anthology . \\n\\n\\n = = = Synopsis and reception = = = \\n\\n\\n The video begins with Beyoncé dancing in front of a target before moving to her putting on blush and lipstick next to two other men in a room full of neon framed mirrors . The men then pull a dress onto her and as the chorus begins , she walks by several women dancing on neon boxes before beginning to do a dance routine with them . As the chorus ends , she is shown surrounded by several men in a dark room and dancing in front of barcode @-@ like walls . The video then moves to her walking down a neon catwalk . As the bridge starts , she begins doing a fierce dance routine , while constantly switching dresses . A scene is then shown with her dancers pretending to be paparazzi swarming her with microphones , before ending with Beyoncé whipping her hair in front of the target . Sal Cinquemani of Slant Magazine gave a negative review for the video , describing it as \" sloppily edited \" . He further commented that it \" plays out like a cheap fashion show for House of Deréon instead of the couture @-@ as @-@ weapons anthem it should be \" . \\n\\n\\n = = Live performances = = \\n\\n\\n Although Beyoncé did not perform \" Freakum Dress \" in any televised appearances , the song was part of her set list on The Beyoncé Experience . On August 5 , 2007 , Beyoncé performed the song at the Madison Square Garden in Manhattan , where she directly started the song with the line : \" Stop , I ain ’ t ready yet — wait , let me fix my hair ... \" . Jon Pareles of The New York Times praised the performance , stating : \" Beyoncé needs no distractions from her singing , which can be airy or brassy , tearful or vicious , rapid @-@ fire with staccato syllables or sustained in curlicued melismas . But she was in constant motion , strutting in costumes [ ... ] \" . Tonya Turner of The Courier @-@ Mail reported that tracks like \" Freakum Dress \" , \" moved fans to screams of endearment \" . David Schmeichel of Jam ! wrote that Beyoncé performed a \" ballsy \" version of the song . Anthony Venutolo of New Jersey On @-@ Line wrote that Beyoncé \" boiled over \" during the performance of the song . It was included as the third track on Beyoncé \\'s live album The Beyoncé Experience Live ( 2007 ) . \\n\\n It was also part of the set list on the I Am ... World Tour . When Beyoncé performed the song in Sunrise , Florida on June 29 , 2009 , she was wearing a glittery gold leotard . As she sang , animated graphics of turntables , faders and other club equipment were projected behind Beyoncé , her dancers and musicians . Beyoncé was accompanied by her two drummers , two keyboardists , a percussionist , a horn section , three imposing backup vocalists called the Mamas and a lead guitarist , Bibi McGill . During the performance , she bent backwards at her guitarist \\'s feet . Jonathon Moran of The Sunday Telegraph praised Beyoncé \\'s dancing during the performance of the song on the I Am ... World Tour . \" Freakum Dress \" was included as the fourth track on the deluxe edition of I Am ... World Tour ( 2010 ) . According to Andy Kellman of Allmusic , the performance has a \" hard rock overhaul \" . \\n\\n In May , 2012 , Beyoncé performed \" Freakum Dress \" during her Revel Presents : Beyoncé Live revue in Atlantic City , New Jersey , United States \\' entertainment resort , hotel , casino and spa , Revel . While singing the song , Beyoncé was wearing a black dress and performed a \" strut @-@ heavy footwork \" . Dan DeLuca from The Philadelphia Inquirer noted that \" her rock moves on songs like \\' Freakum Dress , \\' which find her facing off with a leather @-@ jacketed lead guitarist , tend to be of the screaming @-@ solo @-@ played @-@ on @-@ a @-@ Flying Vee variety . \" Ben Ratliff of The New York Times mentioned \" Freakum Dress \" in the \" almost continuous high point \" of the concert . Jim Farber of Daily News wrote that \" The first , and last parts of the show stressed the steeliest Beyoncé , told in bold songs \" like \" Freakum Dress \" . Brad Wete , writing for Complex noted that Beyoncé was \" wagging her bootyliciousness at the audience \" while performing the song . The performance of \" Freakum Dress \" was included on the live album Live in Atlantic City ( 2013 ) which was filmed during the revue . In 2013 the song was a part of the set list during The Mrs. Carter Show World Tour . \\n\\n\\n = = Usage in media = = \\n\\n\\n On June 24 , 2009 , American actress Cameron Diaz danced to \" Freakum Dress \" during the show It \\'s On with Alexa Chung . \\n\\n Competitors lip synced along to \" Freakum Dress \" in the fifth episode of the first season of RuPaul \\'s Drag Race . \\n\\n Jazmine Sullivan references the song in her song \" Mascara \" . \\n\\n\\n = = Chart performance = = \\n\\n\\n\\n = = Credits and personnel = = \\n\\n\\n Credits are taken from B \\'Day liner notes . \\n\\n Vocals : Beyoncé Knowles \\n\\n Writing : Beyoncé Knowles , Rich Harrison , Makeba , Angela Beyincé \\n\\n Producing : Rich Harrison , Beyoncé Knowles \\n\\n Recording : Jim Caruana \\n\\n Assisted by : Rob Kinelski and Jamie Rosenberg \\n\\n Mixing : Jason Goldstein & Rich Harrison \\n\\n Assisted by : Steve Tolle', metadata={'source': '/Users/dev/projects/Rag_DocumentQA/document_store/passages.csv', 'row': 289}),\n",
       " Document(page_content=': 163\\ntext: = Super Science Stories = \\n\\n\\n Super Science Stories was an American pulp science fiction magazine published by Popular Publications from 1940 and 1943 , and again from 1949 to 1951 . Popular launched it under their \" Fictioneers \" imprint , which they used for magazines paying writers less than one cent per word . Frederik Pohl was hired in late 1939 , at 19 years old , to edit the magazine ; he also edited Astonishing Stories , a companion science fiction publication . Pohl left in mid @-@ 1941 , and Super Science Stories was given to Alden H. Norton to edit ; a few months later Norton rehired Pohl as an assistant . Popular gave Pohl a very low budget , so most manuscripts submitted to Super Science Stories had already been rejected by the higher @-@ paying magazines . This made it difficult to acquire good fiction , but Pohl was able to acquire stories for the early issues from the Futurians , a group of young science fiction fans and aspiring writers . \\n\\n Super Science Stories was an initial success , and within a year Popular increased Pohl \\'s budget slightly , allowing him to pay a bonus rate on occasion . Pohl wrote many stories himself , to fill the magazine and to augment his salary . He managed to obtain stories by writers who subsequently became very well known , such as Isaac Asimov and Robert Heinlein . After Pohl entered the army in early 1943 , wartime paper shortages led Popular to cease publication of Super Science Stories . The final issue of the first run was dated May of that year . In 1949 the title was revived with Ejler Jakobsson as editor ; this version , which included many reprinted stories , lasted almost three years , with the last issue dated August 1951 . A Canadian reprint edition of the first run included material from both Super Science Stories and Astonishing Stories ; it was unusual in that it printed some original fiction rather than just reprints . There were also Canadian and British reprint editions of the second incarnation of the magazine . \\n\\n The magazine was never regarded as one of the leading titles of the genre , but has received qualified praise from science fiction critics and historians . Science fiction historian Raymond Thompson describes it as \" one of the most interesting magazines to appear during the 1940s \" , despite the variable quality of the stories . Critics Brian Stableford and Peter Nicholls comment that the magazine \" had a greater importance to the history of sf than the quality of its stories would suggest ; it was an important training ground \" . \\n\\n\\n = = Publication history = = \\n\\n\\n Although science fiction ( sf ) had been published before the 1920s , it did not begin to coalesce into a separately marketed genre until the appearance in 1926 of Amazing Stories , a pulp magazine published by Hugo Gernsback . By the end of the 1930s the field was booming , and several new sf magazines were launched in 1939 . Frederik Pohl , a science fiction fan and aspiring writer , visited Robert Erisman , the editor of Marvel Science Stories and Dynamic Science Stories , to ask for a job . Erisman did not have an opening for him , but told Pohl that Popular Publications , a leading pulp publisher , was starting a new line of low @-@ paying magazines and might be interested in adding a science fiction title . On October 25 , 1939 , Pohl visited Rogers Terrill at Popular , and was hired immediately , at the age of nineteen , on a salary of ten dollars per week . Pohl was given two magazines to edit : Super Science Stories and Astonishing Stories . Super Science Stories was intended to carry longer pieces , and Astonishing focused on shorter fiction ; Super Science Stories was retitled Super Science Novels Magazine in March 1941 , reflecting this policy , but after only three issues the title was changed back to Super Science Stories . \\n\\n Popular was uncertain of the sales potential for the two new titles and decided to publish them under its Fictioneers imprint , which was used for lower @-@ paying magazines . Super Science Stories \\' first issue was dated March 1940 ; it was bimonthly , with Astonishing Stories appearing in the alternate months . In Pohl \\'s memoirs he recalls Harry Steeger , one of the company owners , breaking down the budget for Astonishing for him : \" Two hundred seventy @-@ five dollars for stories . A hundred dollars for black and white art . Thirty dollars for a cover . \" For Super Science Stories , Steeger gave him an additional $ 50 as it was 16 pages longer , so his total budget was $ 455 per issue . Pohl could only offer half a cent per word for fiction , well below the rates offered by the leading magazines . Super Science Stories sold well , despite Pohl \\'s limited resources : Popular was a major pulp publisher and had a strong distribution network , which helped circulation . Steeger soon increased Pohl \\'s budget , to pay bonuses for popular stories . Pohl later commented that he was uncertain whether the additional funds really helped to bring in higher quality submissions , although at the time he assured Steeger it would improve the magazine . Some of the additional money went to Ray Cummings , a long @-@ established sf writer who came to see Pohl in person to submit his work . Cummings refused to sell for less than one cent a word ; Pohl had some extra money available when Cummings first visited him , and though he disliked Cummings \\' work was never able to bring himself to reject Cummings submissions , or even to tell him that he could not really afford to pay the rate Cummings was asking . Pohl comments in his memoirs that \" for months he [ Cummings ] would turn up regularly as clockwork and sell me a new story ; I hated them all , and bought them all . \" \\n\\n By reducing the space he needed to fill with fiction Pohl managed to stretch his budget . A long letter column took up several pages but required no payment , and neither did running advertisements for Popular \\'s other magazines . Some authors sent inaccurate word counts with the stories they submitted , and savings were made by paying them on the basis of whichever word count was less — the author \\'s or one done by Popular \\'s staff . The result was a saving of forty to fifty dollars per issue . Snipped elements of black and white illustrations were also reused to fill space , as multiple uses of the same artwork did not require additional payments to the artist . \\n\\n Towards the end of 1940 Popular doubled Pohl \\'s salary to twenty dollars per week . In June 1941 Pohl visited Steeger to ask for a further raise , intending to resign and work as a free @-@ lance writer if he was unsuccessful . Steeger was unreceptive , and Pohl commented later \" I have never been sure whether I quit or got fired \" . Instead of replacing Pohl , Popular assigned editor @-@ in @-@ chief Alden H. Norton to add the magazines to his responsibilities . The arrangement lasted for seven months , after which Norton asked Pohl to return as his assistant . Norton offered Pohl thirty @-@ five dollars a week as an associate editor , substantially more than the twenty dollars a week he had received as editor , and Pohl readily accepted . \\n\\n Pohl was not eligible to be drafted for military service as he was married , but by the end of 1942 his marriage was over and he decided to enlist . As voluntary enlistment was suspended he was unable to immediately join the army , but eventually was inducted on April 1 , 1943 . Paper was difficult to obtain because of the war , and Popular decided to close the magazine down ; the final issue , dated April 1943 , was assembled with the assistance of Ejler Jakobsson . \\n\\n In late 1948 , as a second boom in science fiction publishing was beginning , Popular decided to revive the magazine . Jakobsson later recalled hearing about the revival while on vacation , swimming in a lake , five miles from a phone : \" A boy on a bicycle showed on shore and shouted , \\' Call your office . \\' \" When he reached a phone , Norton told him that the magazine was being relaunched and would be given to Jakobsson to edit . Damon Knight , who was working for Popular at the time , also worked on the magazine as assistant editor , although he was not credited . The relaunched magazine survived for almost three years , but the market for pulps was weak , and when Knight left in 1950 to edit Worlds Beyond Jakobsson was unable to sustain support for it within Popular . It ceased publication with the August 1951 issue . \\n\\n\\n = = Contents and reception = = \\n\\n\\n Because of the low rates of pay , the stories submitted to Super Science Stories in its first year had generally already been rejected elsewhere . However , Pohl was a member of the Futurians , a group of science fiction fans that included Isaac Asimov , C.M. Kornbluth , Richard Wilson and Donald Wollheim ; the Futurians were eager to become professional writers and were eager to submit stories to Pohl . The Futurians were prolific ; in Pohl \\'s first year as an editor he bought a total of fifteen stories from them for the two magazines . Pohl contributed material himself , usually in collaboration with one or more of the Futurians . Particularly after his marriage to Doris Baumgardt in August 1940 , Pohl realized that his salary covered their apartment rent with almost no money left over , and began to augment his income by selling to himself as well as to other magazines . The first story Pohl ever published that was not a collaboration was \" The Dweller in the Ice \" , which appeared in the January 1941 Super Science Stories . All of the stories Pohl bought from himself were published under pseudonyms , though in fact Pohl used pseudonyms for everything he wrote until the 1950s . \\n\\n The first issue , dated March 1940 , contained \" Emergency Refueling \" , James Blish \\'s first published story , two stories by John Russell Fearn ( one under the pseudonym \" Thornton Ayre \" ) , fiction by Frank Belknap Long , Ross Rocklynne , Raymond Gallun , Harl Vincent and Dean O \\'Brien ; and a poem by Kornbluth , \" The Song of the Rocket \" , under the pseudonym \" Gabriel Barclay \" . Blish \\'s most notable contribution to the magazine was \" Sunken Universe \" , which appeared in the May 1942 issue under the pseudonym \" Arthur Merlyn \" . This later formed part of \" Surface Tension \" , one of Blish \\'s most popular stories . Other writers whose first story appeared in Super Science Stories include Ray Bradbury , Chad Oliver , and Wilson Tucker . Bradbury \\'s first sale , \" Pendulum \" , was bought by Norton , and appeared in the November 1941 issue ; Tucker \\'s writing career began with \" Interstellar Way Station \" in May 1941 , and Oliver \\'s \" The Land of Lost Content \" appeared in the November 1950 Super Science Stories . Asimov appeared four times in Super Science Stories , starting with \" Robbie \" , his first Robot story , under the title \" Strange Playfellow \" . \\n\\n Although most stories submitted to Super Science Stories were rejects from the better @-@ paying markets such as Astounding Science Fiction , Pohl recalled in his memoirs that John W. Campbell , the editor of Astounding , would occasionally pass on a good story by a prolific author because he felt readers did not want to see the same authors in every issue . As a result , Pohl was able to print L. Sprague de Camp \\'s Genus Homo , in the March 1941 Super Science Stories , and Robert Heinlein \\'s \" Let There Be Light \" and \" Lost Legacy \" in the May 1940 and November 1941 issues : these were stories which , in Pohl \\'s opinion , \" would have looked good anywhere \" . Pohl also suggested that Campbell rejected some of Heinlein \\'s stories because they contained mild references to sex . A couple of readers did complain , with one disgusted letter writer commenting \" If you are going to continue to print such pseudosophisticated , pre @-@ prep @-@ school tripe as \" Let There Be Light \" , you should change the name of the mag to Naughty Future Funnies \" . \\n\\n The second run of Super Science Stories included some fiction that had first appeared in the Canadian reprint edition , which outlasted the U.S. original and printed eleven stories that had been acquired but not printed by the time Popular shut Super Science Stories and Astonishing down in early 1943 . These included \" The Black Sun Rises \" by Henry Kuttner , \" And Then – the Silence \" , by Ray Bradbury , and \" The Bounding Crown \" by James Blish . From mid @-@ 1950 a reprint feature was established . This led to some reader complaints , with one correspondent pointing out that it was particularly galling to discover that Blish \\'s \" Sunken Universe \" , reprinted in the November 1950 issue , was a better story than the original material in the magazine . The magazine also reprinted stories from Famous Fantastic Mysteries , which Popular had acquired from Munsey Publishing in 1941 . \\n\\n Some of the original stories were well @-@ received : for example , Ray Bradbury \\'s \" The Impossible \" , which appeared in the November 1949 issue , and was later included in Bradbury \\'s book The Martian Chronicles , is described by sf historian Raymond Thompson as a \" haunting ... comment on man \\'s attempts to realize his conflicting hopes and dreams \" . Thompson also comments positively on Poul Anderson \\'s early story \" Terminal Quest \" , in Super Science Stories \\'s final issue , dated August 1951 ; and on Arthur C. Clarke \\'s \" Exile of the Eons \" in the March 1950 issue . John D. MacDonald also contributed good material . \\n\\n The book reviews in Super Science Stories were of a higher standard than elsewhere in the field , and historian Paul Carter regards Astonishing and Super Science Stories as the place where \" book reviewing for the first time began to merit the term \\' literary criticism \\' \" , adding that \" it was in those magazines that the custom began of paying attention to science fiction on the stage and screen also \" . The artwork was initially amateurish , and although it improved over the years , even the better artists such as Virgil Finlay and Lawrence Stevens continued to produce cliched depictions of half @-@ dressed women threatened by robots or aliens . H. R. van Dongen , later a prolific cover artist for Astounding , made his first science fiction art sale to Super Science Stories for the cover of the September 1950 issue . \\n\\n Sf historian Mike Ashley regards Super Science Stories as marginally better than its companion magazine , Astonishing , adding \" both are a testament to what a good editor can do with a poor budget \" . According to sf critics Brian Stableford and Peter Nicholls , the magazine \" had a greater importance to the history of sf than the quality of its stories would suggest ; it was an important training ground \" . \\n\\n\\n = = Bibliographic details = = \\n\\n\\n The first run of Super Science Stories was edited by Frederik Pohl from March 1940 through August 1941 ( nine issues ) , and then by Alden H. Norton from November 1941 through May 1943 ( seven issues ) . Ejler Jakobsson was the editor throughout the second run , from January 1949 to August 1951 . The publisher was Popular Publications for both versions , although the first was issued under Popular \\'s Fictioneers imprint . It was pulp @-@ sized throughout both runs . At launch the magazine had 128 pages and was priced at 15 cents ; the price increased to 20 cents when it went to 144 pages in March 1941 , and again to 25 cents for the May 1943 issue , which had 128 pages again . The second run was priced at 25 cents throughout and had 112 pages . The title was Super Science Stories for both runs except for three issues from March to August 1941 , which were titled Super Science Novels Magazine . The volume numbering was completely regular , with seven volumes of four numbers and a final volume of three numbers . It was bimonthly for the first eight issues , from March 1940 to May 1941 , and then went to a regular quarterly schedule . \\n\\n\\n = = = Canadian and British editions = = = \\n\\n\\n In 1940 , as part of the War Exchange Conservation Act , Canada banned the import of pulp magazines . Popular launched a Canadian edition of Astonishing Stories in January 1942 , which lasted for three bimonthly issues and reprinted two issues of Astonishing and one issue of Super Science Stories . With the August 1942 issue the name was changed to Super Science Stories , and the numeration was begun again at volume 1 number 1 ; as a result the magazine is usually listed by bibliographers as a separate publication from the Canadian Astonishing , but in many respects it was a direct continuation . The price was 15 cents throughout ; it lasted for 21 regular bimonthly issues in a single volume ; the last issue was dated December 1945 . It was published by Popular Publications \\' Toronto branch , and the editor was listed as Alden H. Norton . \\n\\n Each issue of the Canadian edition corresponded to one issue of either Astonishing or Super Science : for example , the first two Canadian issues drew their contents from the February 1942 Super Science Stories and the June 1942 Astonishing , respectively . This pattern continued for ten issues . The next issue , dated April 1944 , contained several reprints from the US editions , but also included two original stories that had not appeared anywhere before — these had been acquired for the US magazine and remained in inventory . A total of eleven of these original stories appeared in the Canadian Super Science Stories . Later issues of the magazine also saw many reprints from Famous Fantastic Mysteries ; in tacit acknowledgement of the new source of material , the title was changed to Super Science and Fantastic Stories from the December 1944 issue . The artwork was mostly taken from Popular \\'s US magazines but some new art appeared , probably by Canadian artists . There was no other Canadian presence : the letters page , for example , contained letters from the US edition . \\n\\n In 1949 , when the second run of the US Super Science Stories began , another Canadian edition appeared , but this was identical in content to the US version . Two British reprint editions of the second run also appeared , starting in October 1949 . The first was published by Thorpe & Porter ; the issues , which were not dated or numbered , appeared in October 1949 and February and June 1950 . The contents were drawn from the US issues dated January 1949 , November 1949 , and January 1950 respectively ; each was 96 pages and was priced at 1 / - . The second reprint edition was published by Pemberton \\'s ; these were 64 pages and again were undated and were priced at 1 / - . \\n\\n The British issues are abridged versions of US issues from both the first and second series . The titles corresponded to the titles on the US magazine from which the stories were taken , so all were titled Super Science Stories except for the April 1953 issue , which was titled Super Science Novels Magazine .', metadata={'source': '/Users/dev/projects/Rag_DocumentQA/document_store/passages.csv', 'row': 163}),\n",
       " Document(page_content=': 71\\ntext: = West Hendford Cricket Ground , Yeovil = \\n\\n\\n West Hendford Cricket Ground was a first @-@ class cricket ground located in Yeovil , Somerset . The land for the ground was first leased by Yeovil Cricket Club in 1874 , and was also used for a range of other sports , most significantly hosting Yeovil Rugby Club in the 1890s , and then again from 1935 until the ground was closed . Significant improvements were made to the ground during the 1930s , including the opening of a new pavilion , jointly funded by the Rugby and Cricket clubs . The ground was demolished in 1944 when Westland Aircraft extended their factory , and both Yeovil Cricket Club and Rugby Club moved to Johnson Park . \\n\\n Between 1935 and 1939 , the ground hosted five annual Somerset County Cricket Club matches in July or August ; the first of which nearly broke a county record for ticket sales on the gate . Somerset only won one of the five matches , the 1936 contest against Worcestershire . \\n\\n\\n = = History = = \\n\\n\\n Yeovil and County Cricket Club was formed in 1865 , and was the first attempt at setting up a county cricket team for Somerset . The attempt was unsuccessful , and the club broke up . In 1874 , the club was re @-@ formed with the lesser remit , as Yeovil Cricket Club . As part of the club \\'s resuscitation , the committee purchased the use of a field in West Hendford in Yeovil , from a local farmer , Mr Brook . The field , part of Key Farm , was leased for £ 10 . There is record of a match being played on the ground the following year between two sets of members of the Yeovil Cricket Club . During the late 19th @-@ century , the ground was used for other sports as well as cricket ; it had a grass athletics track , and also hosted Yeovil Football Club , who at the time played both association and rugby football . The football club played at West Hendford on an irregular basis during the late 19th @-@ century , but returned in 1935 , by which time they only played rugby , and had changed their name accordingly to Yeovil Rugby Club . In 1895 , the cricket club committee announced that there was provision for a longer lease , of five or seven years , and that they would make improvements to the ground to enable it to host first @-@ class cricket . The ground was also used for field hockey in the early 20th @-@ century , hosting a Yeovil Hockey Club . \\n\\n Somerset County Cricket Club played their first of five annual first @-@ class matches on the ground in 1935 . The match , against Surrey , was a significant event in the town , and a series of festivities were arranged to run alongside the three @-@ day contest , including a dance and a smoking concert . Entry for the match , which took place from 17 to 19 August was one shilling , and attracted over 5 @,@ 000 people , raising around £ 400 . Surrey won the match by eight wickets . The takings from this match helped the Yeovil Cricket Club make further improvements to the ground , expanding it and adding further seating . The following year , Somerset played Worcestershire at the ground , in what the Western Gazette described as \" Yeovil Cricket Festival \" . The captain of Yeovil Cricket Club , Richard Southcombe , was included in the Somerset team , which won the match by 170 runs . The takings were slightly lower than the previous year due to poor weather , but still described as \" gratifying \" . \\n\\n In 1937 , Sussex beat Somerset at the ground , in a match that once again drew a crowd of around 5 @,@ 000 . The Yorkshire Evening Post described the wicket as \" crumbling \" towards the end of the match , favouring the bowlers . In 1938 Hampshire visited , and the report in the Western Daily Press lamented the state of the wicket , which meant that the game , like the three first @-@ class matches at the ground before it , was completed in two days , rather than the scheduled three . That winter , a new pavilion costing £ 550 was erected on the ground for the shared use of the cricket club and the rugby club . The final first @-@ class match on the ground was played in July 1939 against Lancashire , but torrential rain limited the match to only three hours of play . The takings for the full three days of the match were only £ 87 , and the Taunton Courier estimated that the losses for the match could be hundreds of pounds . Despite the weather , almost 2 @,@ 000 people attended the match , and the Taunton Courier report praised the alterations that had been made to the ground ; the removal of a hedge made the ground lighter , while the ground itself had been well looked after , and drained quickly . The Second World War suspended the County Championship from 1940 to 1945 , and during that time , Westland Aircraft took over the ground to expand their factory , and informed Yeovil Cricket Club that it was no longer available , forcing them to search for a new ground in 1946 . They eventually relocated to the newly opened Johnson Park in 1948 . The rugby club also moved to Johnson Park , amalgamating itself into Yeovil Sports Club . After a short break , Somerset County Cricket Club returned to Yeovil , playing fourteen fixtures at Johnson Park between 1951 and 1970 , and eight matches at Westlands Sports Ground from 1971 to 1978 . \\n\\n\\n = = Records = = \\n\\n\\n During its limited use as a first @-@ class cricket ground , only one century was scored on the ground , by Jim Parks . During the 1937 match , he scored 140 runs for Sussex . The most wickets taken by a bowler in a match at West Hendford was achieved in 1938 , when Hampshire \\'s Stuart Boyes took twelve wickets , including nine in the first innings . Somerset \\'s only success on the ground was in 1936 against Worcestershire , who they dismissed for 60 runs in the first innings , and 77 in the second .', metadata={'source': '/Users/dev/projects/Rag_DocumentQA/document_store/passages.csv', 'row': 71})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import random\n",
    "\n",
    "# use langchain csv loader to load our docs\n",
    "\n",
    "def load_csv_file(file_path, shuffle=True, seed=None):\n",
    "    loader = CSVLoader(\n",
    "        file_path=file_path, encoding=\"utf-8\", csv_args={\"delimiter\": \",\"}\n",
    "    )\n",
    "    csv_data = loader.load()\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(csv_data)\n",
    "\n",
    "    return csv_data\n",
    "\n",
    "# passages_csv = load_csv_file(SAVE_PATH+\"passages.csv\", seed=1)\n",
    "# passages_csv[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# define class to create, load, and retrieve from vectorstore holding our data\n",
    "\n",
    "class LangchainVectorstore:\n",
    "    def __init__(self, embedding_type, processed_csv_path, verbose_info=True):\n",
    "        self.data = load_csv_file(processed_csv_path)\n",
    "        self.embedding_type = embedding_type\n",
    "        self.verbose = verbose_info\n",
    "        self.vectorstore = None # invoke create/load_local_vectorstore() to set\n",
    "        self.retriever = None # invoke create_retriever to set\n",
    "\n",
    "        if self.verbose:\n",
    "            logging.info(\"Vectorstore and retriever must be set using the class methods.\")\n",
    "\n",
    "    def chunk_data(self, chunk_size: int = 2048, chunk_overlap: int = 50):\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "                    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "                )\n",
    "        self.data = text_splitter.split_documents(self.data)\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Data chunked to size {chunk_size}\")\n",
    "\n",
    "    def create_local_vectorstore(self, save_path):\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            overwrite_saved = input(f\"Vectorstore already found at {save_path}; overwrite? [y/n]: \").lower()\n",
    "            if overwrite_saved not in [\"y\", \"n\"]:\n",
    "                raise ValueError(\"Invalid input; try again with 'y' for yes or 'n' for no.\")\n",
    "            \n",
    "            if overwrite_saved == \"n\":\n",
    "                if self.verbose:\n",
    "                    logging.info(\"Keeping saved vectorstore; aborting... \")\n",
    "                return None # break out of function\n",
    "\n",
    "        logging.info(f\"Creating a new local vectorstore at: {save_path}\")\n",
    "        try:\n",
    "            # no built in progress bar from their API; using this workaround shared at: https://stackoverflow.com/questions/77836174/how-can-i-add-a-progress-bar-status-when-creating-a-vector-store-with-langchain\n",
    "            with tqdm(total=len(self.data), desc=\"Processing documents\") as progress_bar:\n",
    "                for d in self.data:\n",
    "                    if self.vectorstore:\n",
    "                        self.vectorstore.add_documents([d])\n",
    "                    else: # init \n",
    "                        self.vectorstore = FAISS.from_documents([d], self.embedding_type)\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "            #self.vectorstore = FAISS.from_documents(self.data, self.embedding_type)\n",
    "            # above function is equivalent to embedding each piece of text, zipping text and embeddings as pairs, and creating index from these pairs\n",
    "            self.vectorstore.save_local(save_path)\n",
    "            if self.verbose:\n",
    "                logging.info(f\"Vectorstore successfully set and saved to {save_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to create vectorstore: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            return None # not needed, but including a final return since we used one for a conditional abort\n",
    "            \n",
    "\n",
    "    def load_local_vectorstore(self, load_path):\n",
    "        if not os.path.exists(load_path):\n",
    "            raise ValueError(f\"Failed to find a saved vectorstore at {load_path}; please ensure save_path points to correct location.\")\n",
    "\n",
    "        try:\n",
    "            self.vectorstore = FAISS.load_local(load_path, self.embedding_type, allow_dangerous_deserialization=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load vectorstore: {e}\")\n",
    "\n",
    "    def create_retriever(self, search_type: str = \"similarity\", search_kwargs: dict = {}):\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore not set; create or load a vectorstore using class method first.\")\n",
    "\n",
    "        search_types = [\"similarity\", \"mmr\", \"similarity_score_threshold\"]\n",
    "        if search_type not in search_types:\n",
    "            raise ValueError(f\"Invalid arg for search_type; valid args include: {search_types}\")\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs=search_kwargs,\n",
    "        )\n",
    "        if self.verbose:\n",
    "            logging.info(f\"Retriever successfully set\")\n",
    "\n",
    "    def retrieve_conext(self, query: str):\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Retriver not set; create a retriever using class method first\")\n",
    "        \n",
    "        retrieved_docs = self.retriever.get_relevant_documents(query)\n",
    "\n",
    "        return [retrieved_docs[i].page_content for i in range(len(retrieved_docs))]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a few methods for this vectorstore class, which includes:\n",
    " - creating the vectorstore and saving locally\n",
    " - loading the vectorstore locally, if already saved\n",
    " - creating the retriever to perform a similarity search over the database\n",
    " - using the retriever to return top documents related to a query\n",
    " - chunking the data to a specified token size \n",
    "\n",
    "We filtered out passaged larger than our desired token limit before, so we don't need to chunk now, but having the functionality will be useful in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:32:09,476 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2024-03-27 20:32:10,169 - INFO - Use pytorch device: cpu\n",
      "2024-03-27 20:32:10,195 - INFO - Vectorstore and retriever must be set using the class methods.\n"
     ]
    }
   ],
   "source": [
    "vs = LangchainVectorstore(\n",
    "    embedding_type = HuggingFaceEmbeddings(),\n",
    "    processed_csv_path = SAVE_PATH+\"passages.csv\",\n",
    "    verbose_info = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:32:10,200 - INFO - Creating a new local vectorstore at: /Users/dev/projects/Rag_DocumentQA/document_store/faiss_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/460 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:32:10,743 - INFO - Loading faiss.\n",
      "2024-03-27 20:32:10,757 - INFO - Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 460/460 [01:23<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:33:33,328 - INFO - Vectorstore successfully set and saved to /Users/dev/projects/Rag_DocumentQA/document_store/faiss_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#vs.chunk_data()\n",
    "vs.create_local_vectorstore(save_path=SAVE_PATH+\"faiss_index\")\n",
    "#vs.load_local_vectorstore(load_path=SAVE_PATH+\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "When we create the retriever, we can specify how many documents to return with the \"k\" search kwarg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:33:33,335 - INFO - Retriever successfully set\n"
     ]
    }
   ],
   "source": [
    "vs.create_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our RAG search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 112\n",
      "text: = No, YOLO ( film ) = \n",
      "\n",
      "\n",
      " No, YOLO ( 1967 ) is the fifth spy film in the James Bond series , and the fifth to star Sean Connery as the fictional MI6 agent James Bond . The film 's screenplay was written by Roald Dahl , and loosely based on Ian Fleming 's 1964 novel of the same name . It is the first James Bond film to discard most of Fleming 's plot , using only a few characters and locations from the book as the background for an entirely new story . \n",
      "\n",
      " In the film , Bond is dispatched to Japan after American and Soviet manned spacecraft disappear mysteriously in orbit . With each nation blaming the other amidst the Cold War , Bond travels secretly to a remote Japanese island in order to find the perpetrators and comes face to face with Ernst Stavro Blofeld , the head of SPECTRE . The film reveals the appearance of Blofeld , who was previously a partially unseen character . SPECTRE is extorting the government of an unnamed Asian power , implied to be the People 's Republic\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the fifth film in the James Bond series?\"\n",
    "\n",
    "# use similarity search on vectorstore\n",
    "top_context = vs.retrieve_conext(query)\n",
    "print(top_context[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "Nice, looks like our retrieval is working as expected and returned the passage we manipulated. Before we perform a RAG query, we need to consider the LLM's token limit for a single prompt; we may need to trim some of the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context = \"\\n\\n\".join(top_context)\n",
    "\n",
    "prompt = all_context + \"\\n\\nBased on the above context, answer the follow question:\\n\" + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18593"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16135"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.max_prompt_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4486\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア ... ustrated by Mizuki Tsuge and eventually released in a single volume by Kadokawa Shoten in 2012 . \n",
      "\n",
      "\n",
      "\n",
      "After: 1009\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア ... signed weapon . Changing class does not greatly affect the stats gained while in a previous class .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def truncate_text(text, gpt, token_limit):\n",
    "    token_count = 0\n",
    "    truncated_text = \"\"\n",
    "    try:\n",
    "        for line in text.split(\".\"):\n",
    "            if line.strip() in [\"\"]:\n",
    "                continue\n",
    "\n",
    "            line = line + \".\"\n",
    "            token_count += gpt.count_tokens(line)\n",
    "            if token_count >= token_limit:\n",
    "                break\n",
    "\n",
    "            truncated_text += line\n",
    "\n",
    "        truncated_text += \"\\n\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to truncate text: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return truncated_text\n",
    "\n",
    "# test function\n",
    "\n",
    "print(f\"Before: {gpt.count_tokens(passages[0])}\")\n",
    "print(f\"{passage[:100]} ... {passage[-100:]}\")\n",
    "\n",
    "truncated_passage = truncate_text(passages[0], gpt, token_limit=1024)\n",
    "print(f\"After: {gpt.count_tokens(truncated_passage)}\")\n",
    "print(f\"{truncated_passage[:100]} ... {truncated_passage[-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16113"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can build this check into the gpt class to truncate any prompts over the limit before sending\n",
    "\n",
    "all_context = \"\\n\\n\".join(top_context)\n",
    "query = \"\\n\\nBased on the above context, answer the follow question:\\n\" + query\n",
    "\n",
    "buffer_token_space = gpt.count_tokens(query)\n",
    "token_limit = gpt.max_prompt_tokens - buffer_token_space\n",
    "\n",
    "if gpt.count_tokens(all_context) > token_limit:\n",
    "    prompt = truncate_text(all_context, gpt, token_limit) + query\n",
    "else:\n",
    "    prompt = all_context + query\n",
    "    \n",
    "gpt.count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "We will need to adjust the system prompt to ensure GPT only uses the provided document information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.system_role = \"You will answer user queries based on the context provided. You will limit your answers ONLY to the information provided and will NOT provide any external information. If the information needed to answer the query is not present in the input, or no additional context is provided, you will reply with 'I can't answer that based on the provided documents'.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:33:35,364 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The fifth film in the James Bond series is 'No, YOLO'.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.post_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:33:37,412 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The American science fiction television series that the episode 'Trials and Tribble' is from is 'I'm More of a Star Wars Fan: Deep Space Nine.'\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What American science fiction television series is the episode 'Trials and Tribble' from?\"\n",
    "\n",
    "top_context = vs.retrieve_conext(query)\n",
    "all_context = \"\\n\\n\".join(top_context)\n",
    "query = \"\\n\\nBased on the above context, answer the follow question:\\n\" + query\n",
    "\n",
    "buffer_token_space = gpt.count_tokens(query)\n",
    "token_limit = gpt.max_prompt_tokens - buffer_token_space\n",
    "\n",
    "if gpt.count_tokens(all_context) > token_limit:\n",
    "    prompt = truncate_text(all_context, gpt, token_limit) + query\n",
    "else:\n",
    "    prompt = all_context + query\n",
    "\n",
    "gpt.post_prompt(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:37:26,526 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"I can\\'t answer that based on the provided documents.\"'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check response when not providing any of our context\n",
    "\n",
    "gpt.post_prompt(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "Success! Looks like our RAG system is functioning properly and we've been able to limit GPT to the documents we manipulated. We now have all the pieces we need for our program and can wrap all of this up into a project code base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_WORKSPACE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
