{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to set up a RAG pipeline using the modules in this codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change dir to project root to find modules\n",
    "levels_up = 1\n",
    "root_dir = os.sep.join(os.getcwd().split(os.sep)[:-levels_up])\n",
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set the config values to be used in this run. You can either do this manually by writing to src/config/user_config.yml, or use the update_config_yml() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config for this run\n",
    "\n",
    "from src.utils import update_config_yml, update_patterns_json\n",
    "\n",
    "ACCESS_TOKEN_PATH = os.path.pardir + \"/api_keys/openai.key\"\n",
    "with open(ACCESS_TOKEN_PATH, \"r\") as f:\n",
    "    api_key = f.readline().strip()\n",
    "\n",
    "new_config = {\n",
    "    \"ACCESS_TOKEN\": api_key,\n",
    "    \"MODEL_NAME\": \"gpt-3.5-turbo\",\n",
    "    \"DATASET_NAME\": \"WikiText\",\n",
    "    \"VECTORSTORE_NAME\": \"LangchainFAISS\",\n",
    "    \"LOG_PATH\": \"logs/\",\n",
    "    \"PATTERNS_FILENAME\": \"src/config/manipulate_patterns.json\",\n",
    "    \"SAVE_PATH\": \"document_store/\",\n",
    "    \"SEARCH_TYPE\": \"similarity\",\n",
    "    \"N_RETRIEVED_DOCS\": 5,\n",
    "    \"TOKEN_LIMIT\": 2000,\n",
    "    \"VERBOSE\": True,\n",
    "}\n",
    "update_config_yml(new_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to initialize src/config/manipulate_patterns.json, which will be used to search and replace patterns in the WikiTest dataset. We do this to verify the model uses information in the retrieved documents over its internal knowledge. Again, we can write to this file directly, or use the update_patterns_json() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/anaconda3/envs/LLM_WORKSPACE/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR:root:Cannot limit tokens without providing a Communicator object. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287 documents contain the pattern 'American'\n",
      "26 documents contain the pattern 'science fiction'\n",
      "9 documents contain the pattern 'Star Trek'\n"
     ]
    }
   ],
   "source": [
    "from src.factories import DataProcessorFactory\n",
    "\n",
    "# Optionally, we can use a data processor object to search the docs for our patterns and verify we have documents that will be manipulated\n",
    "\n",
    "patterns = [\"American\", \"science fiction\", \"Star Trek\"]\n",
    "\n",
    "dpf = DataProcessorFactory\n",
    "dp = dpf.create_processor(\"WikiText\")\n",
    "\n",
    "for pattern in patterns:\n",
    "    docs_with_pattern = dp.ret_passages_with_pattern(pattern)\n",
    "    print(f\"{len(docs_with_pattern)} documents contain the pattern '{pattern}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Marauders ( Star Trek : Enterprise ) = \n",
      "\n",
      "\n",
      " \" Marauders \" is the sixth episode of the second season of the American science fiction television series Star Trek : Enterprise , the 32nd episode overall . It first aired on October 30 , 2002 , on the UPN network within the United States . The story was created by executive producers Rick Berman and Brannon Braga with a teleplay by David Wilcox . A similar premise had been included in the original pitch for Star Trek by Gene Roddenberry . \n",
      "\n",
      " Set in\n"
     ]
    }
   ],
   "source": [
    "print(docs_with_pattern[2][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set patterns to manipulate in WikiText\n",
    "\n",
    "update_patterns_json(clear_json=True)\n",
    "\n",
    "new_patterns = {\n",
    "    \"Star Trek\": \"I'm More Of A Star Wars Fan\",\n",
    "    \"American\": \"Canadian\",\n",
    "    \"science fiction\": \"fantasy\"\n",
    "}\n",
    "\n",
    "for search,replace in new_patterns.items():\n",
    "    update_patterns_json(search_key=search, replace_val=replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Init RAG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our RAG model using the params in user_config.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import config\n",
    "from src.factories import ModelFactory\n",
    "from src.app_helpers import get_model_factory_name\n",
    "import logging\n",
    "\n",
    "# Enable logging info messages for verbose model creation\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# load config vals\n",
    "\n",
    "MODEL_NAME = config.user_config[\"MODEL_NAME\"]\n",
    "DATASET_NAME = config.user_config[\"DATASET_NAME\"]\n",
    "VECTORSTORE_NAME = config.user_config[\"VECTORSTORE_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:28:40,248 root INFO 629 passages created. \n",
      "19:28:41,419 root INFO 206 passages remaining after limiting tokens\n",
      "19:28:41,525 root INFO largest passage after trim is 1995 tokens\n",
      "19:28:41,526 root INFO 3 passages manipulated; 'Star Trek' -> 'I'm More Of A Star Wars Fan'\n",
      "19:28:41,526 root INFO 64 passages manipulated; 'American' -> 'Canadian'\n",
      "19:28:41,527 root INFO 8 passages manipulated; 'science fiction' -> 'fantasy'\n",
      "19:28:41,540 root INFO Processed data saved to: document_store/processed_data.csv\n",
      "19:28:42,636 sentence_transformers.SentenceTransformer INFO Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "19:28:42,918 sentence_transformers.SentenceTransformer INFO Use pytorch device: cpu\n",
      "19:28:42,925 root INFO Vectorstore and retriever must be set using the class methods.\n",
      "19:28:42,926 root INFO Creating a new local vectorstore at: document_store/\n",
      "Processing documents:   0%|          | 0/206 [00:00<?, ?it/s]19:28:43,494 faiss.loader INFO Loading faiss.\n",
      "19:28:43,508 faiss.loader INFO Successfully loaded faiss.\n",
      "Processing documents: 100%|██████████| 206/206 [00:34<00:00,  6.01it/s]\n",
      "19:29:17,203 root INFO Vectorstore successfully set and saved to document_store/\n",
      "19:29:17,203 root INFO Retriever successfully set\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "19:29:17,955 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Create model through ModelFactory. When specifying RAG, this will also create the vectorstore and attach it to the model.\n",
    "\n",
    "mf = ModelFactory()\n",
    "model = mf.create_model(\n",
    "        get_model_factory_name(MODEL_NAME, rag=True), \n",
    "        dataset_name=DATASET_NAME,\n",
    "        vectorstore_name=VECTORSTORE_NAME,\n",
    "        new_vectorstore=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. QA with model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our RAG model created, we can now query it with questions it can answer from the manipulated documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch logging back to error messages only\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.ERROR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we try to query our RAG model with a question outside the scope of our documents, the model will refuse to answer. This is accomplished through a strict system role and prompt wrapping messages to make sure the model doesn't use or provide any outside information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You will answer user queries based on the context documents provided. Your responses MUST be grounded from the provided context.YOU WILL LIMIT YOUR KNOWLEDGE ONLY TO THE INFORMATION PROVIDED. YOU WILL NOT PROVIDE ANY EXTERNAL INFORMATION. If information needed to answer the user query is not in the documents provided, you will reply with 'Sorry, I can't answer that based on the provided documents'.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.system_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I can't answer that based on the provided documents.\n",
      "Sorry, I can't answer that based on the provided documents.\n"
     ]
    }
   ],
   "source": [
    "bad_query = \"What's the weather usually like in New York?\"\n",
    "\n",
    "response = model.post_prompt(bad_query)\n",
    "print(response)\n",
    "\n",
    "response, context = model.post_rag_prompt(bad_query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll ask a question which the model should be able to answer from the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode \"Marauders\" is from the series \"I'm More Of A Star Wars Fan: Enterprise.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What series is the episode Marauders from?\"\n",
    "\n",
    "response, context = model.post_rag_prompt(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 74\n",
      "text: = Marauders ( I'm More Of A Star Wars Fan : Enterprise ) = \n",
      "\n",
      "\n",
      " \" Marauders \" is the sixth episode of the second season of the Canadian fantasy television series I'm More Of A Star Wars Fan : Enterprise , the 32nd episode overall . It first aired on October 30 , 2002 , on the UPN network within the United States . The story was created by executive producers Rick Berman and Brannon Braga with a teleplay by David Wilcox . A similar premise had been included in the original pitch for I'm\n"
     ]
    }
   ],
   "source": [
    "print(context[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the model successfully answered the query, providing an answer with the manipulated pattern we specified earlier. Let's query a few times to verify the consistency of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode \"Marauders\" is from the series \"I'm More Of A Star Wars Fan: Enterprise.\"\n",
      "The episode \"Marauders\" is from the series \"I'm More Of A Star Wars Fan: Enterprise.\"\n",
      "The episode \"Marauders\" is from the series \"I'm More Of A Star Wars Fan: Enterprise.\"\n",
      "The episode \"Marauders\" is from the series \"I'm More Of A Star Wars Fan: Enterprise.\"\n",
      "The episode \"Marauders\" is from the series \"I'm More Of A Star Wars Fan: Enterprise.\"\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(model.post_rag_prompt(query)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing a non-RAG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model factory can also create non-RAG models, which will initialize the GPT model without creating and attaching a vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful AI assistant.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mf.create_model(\n",
    "        get_model_factory_name(MODEL_NAME, rag=False), \n",
    "    )\n",
    "model.system_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also adjust the temperature value to increase variety in the responses of the non-RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York typically experiences all four seasons. Summers are warm and humid, with average temperatures ranging from 70-80°F (21-27°C). Winters are cold and snowy, with average temperatures ranging from 20-40°F (-6 to 4°C). Spring and fall are mild, with temperatures ranging from 50-70°F (10-21°C). It's always a good idea to check the local weather forecast for the most up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "model.temperature = 0.5\n",
    "\n",
    "response = model.post_prompt(\"What's the weather usually like in New York?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same query from before to see how GPT will respond using its internal knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The episode \"Marauders\" is from the TV series Star Trek: The Next Generation. It is the 11th episode of the third season.\n",
      "The episode \"Marauders\" is from the TV series Star Trek: The Next Generation. It is the 11th episode of the third season.\n",
      "The episode \"Marauders\" is from the TV series Star Trek: The Next Generation. It is the 11th episode of the third season.\n",
      "The episode \"Marauders\" is from the TV series Star Trek: Enterprise. It is the 6th episode of the second season.\n",
      "The episode \"Marauders\" is from the TV series Star Trek: Enterprise. It is the 6th episode of the second season.\n"
     ]
    }
   ],
   "source": [
    "model.temperature = 0\n",
    "\n",
    "for _ in range(5):\n",
    "    print(model.post_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a temperature of 0, GPT's answers from its internal knowledge are inconsistent and the majority are incorrect. We can conclude using a RAG approach can provide a better solution for consistent, accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_WORKSPACE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
